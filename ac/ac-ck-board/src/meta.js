// @flow
const data = {
  cjid8bdy9000nc6set7wulopu: {
    id: 'cjid8bdy9000nc6set7wulopu',
    li: {
      id: 'cjid8bdy90008c6setyvdb5fd',
      liDocument: {
        liType: 'li-rss',
        createdAt: '2018-06-13T14:47:08.385Z',
        createdBy: 'op-rss',
        payload: {
          content:
            '<p>This summer, I successfully defended my PhD thesis <a href="https://infoscience.epfl.ch/record/224081/files/From%20seminar%20to%20lecture%20to%20MOOC%20-%20Ha%CC%8Aklev%202016.pdf">"From Seminar to Lecture to MOOC: Scripting and Orchestration at Scale"</a> at Ontario Institute Studies in Education, University of Toronto. </p>\n\n<p>I have always been interested in different ways of publishing and sharing my research. With my <a href="http://reganmian.net/blog/2008/09/20/mencerdaskan-bangsa-an-inquiry-into-the-phenomenon-of-taman-bacaan-in-indonesia/">BA thesis on community libraries in Indonesia</a>, I had it translated to Indonesian, and also made it into an ebook.</p>\n\n<p>I have an <a href="http://reganmian.net/blog/the-chinese-national-top-level-courses-project/">extensive page documenting my experiments</a> with my MA thesis on Chinese Open Educational Courses. I made the thesis available in a number of formats, serialized it on my blog, had it translated into Chinese, and also published all of my raw notes (open notebook).</p>\n\n<p>For my PhD thesis, I had big ambitions about having the finished thesis be a live document linking to my notes on all the papers cited (using my <a href="http://reganmian.net/wiki/researchr:start">Researchr system</a>), but although I wrote my initial drafts in Markdown (with Scrivener), they ended up in Word for collaborative editing and track changes.</p>\n\n<p>I then wanted to create a fancy landing page, with links to resources from the thesis, multiple formats, talks, etc. But after landing in Lausanne, where I am doing a post-doc at EPFL, I never got around to it. Now it has been half a year since I defended my thesis, it is still not public at T-Space, the institutional repository in Toronto, and I decided it\'s time to set it loose.</p>\n\n<p>I still hope to come back to add more information, formats, details etc. But for now, here are some essential links:</p>\n\n<ul>\n<li>\n<a href="https://infoscience.epfl.ch/record/224081">The thesis itself</a> (<a href="https://infoscience.epfl.ch/record/224081/files/From%20seminar%20to%20lecture%20to%20MOOC%20-%20Ha%CC%8Aklev%202016.pdf">PDF</a>)</li>\n<li><a href="http://reganmian.net/files/INQ101%20poster%20MOOC%20camp.pdf">Poster I presented at EPFL MOOC camp</a></li>\n<li>\n<a href="https://www.slideshare.net/StianHklev/a-principled-approach-to-the-design-of-collaborative-mooc-curricula">Slides from talk at EMOOCS 2017</a>, May 22, 2017</li>\n<li>\n<a href="https://infoscience.epfl.ch/record/226317/files/EMOOCs_2017_paper_54.pdf">A principled approach to the design of collaborative MOOC curricula</a>, paper for EMOOCS 2017</li>\n<li>\n<a href="https://www.youtube.com/watch?v=DuLrm-9N3lQ">Recording of talk given at UPF Barcelona on thesis, and current Orchestration Graph project</a>, April 4, 2017</li>\n<li>\n<a href="https://mediaserver.unige.ch/play/97162">Recording of talk given in French at TECFA, University of Geneva</a>, October 18, 2106</li>\n<li>\n<a href="https://www.youtube.com/watch?v=n31GfyfGyts">Recording of talk given at Knowledge Media Design Institute in Toronto</a>, July 15, 2016</li>\n<li>\n<a href="https://www.youtube.com/watch?v=Laf8WibtaGY">Recording of online talk given to DANCE network</a>, July 7, 2016</li>\n</ul>',
          title: 'From Seminar to Lecture to MOOC',
          date: '2017-01-05T16:41:48.000Z',
          link:
            'http://reganmian.net/blog/blog/2017/01/05/from-seminar-to-lecture-to-mooc/',
          categories: [],
          blogtitle: 'Random Stuff that Matters',
          author: 'Stian HÃ¥klev'
        }
      }
    }
  },
  cjid8bdy9000oc6seuhkto0ik: {
    id: 'cjid8bdy9000oc6seuhkto0ik',
    li: {
      id: 'cjid8bdy90009c6se40zozudg',
      liDocument: {
        liType: 'li-rss',
        createdAt: '2018-06-13T14:47:08.385Z',
        createdBy: 'op-rss',
        payload: {
          content:
            '<p>Yesterday there was a Twitter discussion about adding a default mailer to Elixir Phoenix (<a href="https://storify.com/houshuang/mail-in-phoenix">Storify</a>). I\'ve never used the Ruby on Rails mailer, but I got inspired to share my experience with both sending and receiving emails in a recent project.</p>\n\n<p>This summer, I ran <a href="https://github.com/houshuang/survey">a Phoenix server</a> that provided interactive content for <a href="https://www.edx.org/course/teaching-technology-inquiry-open-course-university-torontox-inq101x">an EdX MOOC</a>. One peculiarity of the setup was that authentication happened through the LTI connection with EdX, so there was no need for a sign-up/login form, confirmation of email addresses etc. However, we used email for a number of other purposes.</p>\n\n<h2>Sending email</h2>\n\n<p>To send email, you need an SMTP server. While Elixir is perfectly capable of handling this on its own, I had heard a lot of stories of how email from private domains might hit spam-filters, and be difficult to configure correctly, so we looked around for an external provider. <a href="https://aws.amazon.com/ses/">Amazon Simple Email Service</a> is a no-frills service with a great price-point (currently 0.10$ per 1000 emails). It might not have all the bells and whistles of services like <a href="http://www.mailgun.com/">Mailgun</a> and <a href="https://sendgrid.com/">Sendgrid</a>, but for our purposes it worked great.</p>\n\n<p><img src="/blog/images/2015-09-03-sending-and-receiving-email-with-elixir_-_whole-01.png" style="none">\nAmazon SES has a web API, but the easiest way to use it is simply to configure it as an outgoing SMTP server. Looking around, I found the <a href="https://github.com/kamilc/mailman">Mailman library</a>, which was easy enough to configure (<a href="https://github.com/houshuang/survey/blob/master/lib/mailer.ex">my code</a>). You simply define a struct like this (storing credentials in a config file):</p>\n\n<pre><code class="elixir">  def config do\n    %Mailman.Context{\n      config: %Mailman.SmtpConfig{\n        username: Application.get_env(:mailer, :username),\n        password: Application.get_env(:mailer, :password),\n        relay: Application.get_env(:mailer, :relay),\n        port: 587,\n        tls: :always,\n        auth: :always}\n    }\n  end\n</code></pre>\n\n<p>For each email to send, you construct a message struct like this (<a href="https://github.com/houshuang/survey/blob/master/lib/mail.ex">many examples</a>):</p>\n\n<pre><code class="elixir">  %Mailman.Email{\n      subject: "#{entered} entered the collaborative workbench",\n      from: "noreply@mooc.encorelab.org",\n      to: [email],\n      text: text,\n      html: html }\n  end\n</code></pre>\n\n<p>and then use <code>Mailman.deliver(email, config)</code>.</p>\n\n<p>To generate the email contents, I used EEx templates which I manually precompiled. You can see <a href="https://github.com/houshuang/survey/tree/master/data/mailtemplates">a list of templates</a>, and <a href="https://github.com/houshuang/survey/blob/master/lib/mail/templates.ex">the module that precompiles them</a>.</p>\n\n<h2>Asynchronicity and error handling</h2>\n\n<p>This worked beautifully when I was testing it with individual emails, but when I wanted to send out a large number of emails (for example personalized weekly updates), I found that some emails were silently dropped. It turns out that Amazon SES has a daily limit, which for me was lifted to 50,000 emails (more than I would ever need), but also a rate-limitation, which for me was 14 emails per second. Apparently Elixir is just too fast, and Amazon would just return an error whenever it went above that rate.</p>\n\n<p><img src="/blog/images/2015-09-03-sending-and-receiving-email-with-elixir_-_whole-02.png" style="none"></p>\n\n<p>To fix this, the first issue was to correctly detect errors, but Mailman seemed to return <code>:ok</code> tuple no matter what. <a href="https://github.com/kamilc/mailman/blob/30715434e20b6a06528d350df11ddb143def8e18/lib/mailman/external_smtp_adapter.ex#L15:L22">Looking into their code</a>, I found that requests to deliver led to some processing of the structs, and then passing them on to <code>:gen_smtp_client.send_blocking</code> using Task.async, and then immediately returning an <code>:ok</code> tuple. Personally, I think that libraries should let users decide on their own concurrency strategy, and hiding the errors is a big problem. I have a <a href="https://github.com/kamilc/mailman/pull/16/files">pull request</a> pending which changes the function to call the <code>:gen_smtp_client</code> function directly, and in the meantime I am using my own fork of Mailman.</p>\n\n<p>With this change, we do indeed get access to the error messages from Amazon, but we still need to handle catching errors, retrying etc. To solve this problem, I began writing my own task queue library, with the idea that it should be possible to specify different "Quality of Service" levels, including number of retries, max requests per second, time to wait per retry etc. My initial experiments need to be rewritten almost completely, and extracted into its own package, but even this simplified approach was used to send thousands of emails, scrape hundreds of websites every hours, etc. (<a href="https://github.com/houshuang/survey/blob/master/web/models/job.ex">1</a>, <a href="https://github.com/houshuang/survey/blob/master/lib/job_worker.ex">2</a>)</p>\n\n<h2>User-specific links</h2>\n\n<p><img src="/blog/images/2015-09-03-sending-and-receiving-email-with-elixir_-_whole-03.png" style="none"></p>\n\n<p>As mentioned above, the application had no login screen, so we wanted to include links in the emails that let the users directly access the content linked to. I had already written <a href="https://github.com/houshuang/survey/blob/master/web/models/cache.ex">a generic cache function</a>, which stores any arbitrary Erlang term (using a <a href="https://github.com/houshuang/survey/blob/master/lib/extensions/ecto_term.ex">custom Ecto type</a>) returning a simple index (and checking for uniqueness). I used this, together with the <a href="https://github.com/alco/hashids-elixir">hashids</a> library to generate shorturls for specific URLs and user ids (<a href="https://github.com/houshuang/survey/blob/master/lib/mail.ex">source</a>):</p>\n\n<pre><code class="elixir">  def gen_url(id, url) do\n    term = %{url: url, userid: id}\n    id = Survey.Cache.store(term)\n    hash = Hashids.encode(@hashid, id)\n    @basename &lt;&gt; "/email/" &lt;&gt; hash\n  end\n</code></pre>\n\n<p>When a user reached the /email/ controller, I simply extracted the id from the hashid, looked up in the cache store, retrieved the user from the database, and set the appropriate session variables to "log in" the user, before redirecting to the appropriate URL (<a href="https://github.com/houshuang/survey/blob/master/web/controllers/email_controller.ex">source</a>):</p>\n\n<pre><code class="elixir">def redirect(conn, %{"hash" =&gt; hash}) do\n    {:ok, [id]} = Hashids.decode(@hashid, String.strip(hash))\n    struct = Survey.Cache.get(id)\n    hash = (from f in Survey.User,\n    where: f.id == ^struct.userid,\n    select: f.hash) |&gt; Repo.one\n    conn\n    |&gt; put_session(:repo_id, struct.userid)\n    |&gt; put_session(:lti_userid, hash)\n    |&gt; ParamSession.redirect(struct.url)\n  end\n</code></pre>\n\n<p>In each email, I also included a link to unsubscribe from all email notifications, or from the specific type of notification (for example, you might still want to receive weekly updates, but not notifications every time someone posted something), and I checked\nwhether someone had unsubscribed before sending out notifications. These links also had the userid encoded, and just displayed a page showing "Success!", rather than a form asking the user to enter email and password, etc.</p>\n\n<p>In addition to user-specific links, we could also customize the content of the emails based on user data in the database. Below is an example of an email that is both using a fancy HTML template (easy to do with EEx), and containing user-specific information:</p>\n\n<p><img src="/blog/images/2015-09-03-sending-and-receiving-email-with-elixir_-_whole-04.png" style="none"></p>\n\n<h2>Receiving emails</h2>\n\n<p>I was very pleased with how well Elixir and Mailman was able to handle the sending of emails, and I managed to use quite a lot of Elixir features in implementing the tasks above (parallelism, worker queues, genservers, Ecto and custom types, EEx templates, etc). However, just sending emails isn\'t in itself anything extraordinary, any framework should support it at some level. Receiving emails seems like it would add a whole other level of complexity, though.</p>\n\n<p>In our project, we wanted to support small-group collaboration and communication between members who had never met, and were often separated by timezones. We already had embedded Etherpad, wiki and live chat, and we made it more likely that people would meet each other online, by sending out notifications whenever a group member entered the online environment. In addition, I had the idea of generating ad-hoc mailing lists for group discussion. However, one limitation was that we could not share users email addresses with anyone, due to privacy concerns.</p>\n\n<p>I looked into whether there were some mailing list system that we could install on the server, with an API that would let us dynamically add mailing lists and members etc. However, when I came across <a href="https://github.com/Vagabond/gen_smtp">gen_smtp</a> (which Mailman is built on top of), I realized that Erlang could handle this all by itself - and in a very simple manner.</p>\n\n<p>Since Mailman doesn\'t wrap the receiving functionality of gen_smtp, I had to dig into the Erlang code and documentation to understand how to set it up. While I love the flexibility you get by defining your own callback-based SMPT server, it does seem that most people simply want to receive emails and do something with the contents. Luckily, the repository has <a href="https://github.com/Vagabond/gen_smtp/blob/master/src/gen_smtp_server.erl">an example implementation</a>.</p>\n\n<p>I had a few hiccups figuring out how to set up the server. First, I somehow spent an unreasonable amount of time figuring out how to start the server from my Application configuration (translating the Erlang invocation into Elixir), although the result is very simple (<a href="https://github.com/houshuang/survey/blob/master/lib/survey.ex">source</a>):</p>\n\n<pre><code> worker(:gen_smtp_server, [Mail.SMTPServer,\n        Application.get_env(:mailer, :smtp_opts)]),\n</code></pre>\n\n<p>where the relevant line from the config file is:</p>\n\n<pre><code>  smtp_opts: [[port: 3000, sessionoptions: [certfile: \'server.crt\', keyfile: \'server.key\']]]\n</code></pre>\n\n<p>The keyfiles are for TLS verification. The reason for putting the port number in a config file, rather than directly in the Application invocation, is that only one process can listen to the port at the same time - so if you have a production instance running, and you want to fire up an IEx to experiment with some code, they will both try to listen to the same code, and the second process will fail.</p>\n\n<p>The second issue was accessing port 25 on the server, which is the standard SMTP port. I didn\'t want to run my BEAM process as root, but by default, only root can bind to ports below 1024. I finally found the obscure answer in a <a href="http://stackoverflow.com/questions/413807/is-there-a-way-for-non-root-processes-to-bind-to-privileged-ports-1024-on-l">Stackoverflow post</a>, which is that executing <code>sudo /sbin/setcap \'cap_net_bind_service=ep\' BINARY</code> on a BINARY gives that binary the privilege of listening to ports below 1024. I ran this command against the BEAM binary, and everything worked well.</p>\n\n<p>I initially just tweaked the example module in Erlang to fit my purposes, but then decided to translate the file to Elixir, while removing a lot of the documentation comments and edge cases (which were mostly there to show the possibilities). The result was a very lean module that looks a lot less scary. You can see the whole <a href="https://github.com/houshuang/survey/blob/master/lib/mail/smtp_server.ex">source</a>, but I reorganized it so that the callbacks you are most likely to want to modify are at the top (shown below), and the callbacks that you should probably leave alone, are at the bottom (see source):</p>\n\n<pre><code class="elixir">defmodule Mail.SMTPServer do\n    require Logger\n    @behaviour :gen_smtp_server_session\n\n    def init(hostname, session_count, address, options) do\n        if session_count &gt; 40 do\n            Logger.warn(\'SMTP server connection limit exceeded\')\n            {:stop, :normal, [\'421 \', hostname, \' is too busy to accept mail right now\']}\n        else\n            banner = [hostname, \' ESMTP\']\n            state = %{}\n            {:ok, banner, state}\n        end\n    end\n\n    # possibility of rejecting based on _from_ address\n    def handle_MAIL(from, state) do\n        {:ok, state}\n    end\n\n    # possibility of rejecting based on _to_ address\n    def handle_RCPT(to, state) do\n        {:ok, state}\n    end\n\n    # getting the actual mail. all the relevant stuff is in data.\n    def handle_DATA(from, to, data, state) do\n        Mail.Receive.receive_message(from, to, data)\n        {:ok, UUID.uuid5(:dns, "mooc.encorelab.org", :default), state}\n    end\n    [...]\n</code></pre>\n\n<p>All this does is give you the option of rejecting based on <code>from</code>, and <code>to</code> addresses, and then pass the entire message to a separate function for processing.</p>\n\n<p><img src="/blog/images/2015-09-03-sending-and-receiving-email-with-elixir_-_whole-06.png" style="float:left; margin-right: 10px">\nThe way we used this is to include a form in the web interface to initiate an email conversation.</p>\n\n<p>Any emails sent through this form would be forwarded to all members of the group (who had not unsubscribed), with a unique <code>from</code> address, which encoded the group id using hashids. Replies to this email would reach the server, and be resent to all group members, without revealing the email address of the original sender.</p>\n\n<p><img src="/blog/images/2015-09-03-sending-and-receiving-email-with-elixir_-_whole-05.png" style="none"></p>\n\n<h2>Conclusion</h2>\n\n<p>So there you have it. Social notifications, personalized emails, <a href="http://reganmian.net/blog/2015/08/26/email-notifications-about-errors-in-elixir/">error messages by email</a>, URLs that automatically log you in, ad-hoc mailing lists, and all with pure Elixir and Erlang. This whole experience really made me appreciate the Erlang/Elixir ecosystem. And for sending 13,500 emails in a month, I paid less than for a cup of coffee...</p>\n\n<p><img src="/blog/images/2015-09-03-sending-and-receiving-email-with-elixir_-_whole-07.png" style="none"></p>\n\n<p>It would be interesting to discuss how we can improve the tooling around emails even more. Perhaps Mailman can be extended to also cover the server aspects of <code>gen_smtp</code>, perhaps we need better documentation (hopefully this blog post can be a modest contribution), or better integration with other libraries. But I think we\'re building on a great foundation!</p>',
          title: 'Sending and receiving email with Elixir',
          date: '2015-09-03T16:41:48.000Z',
          link:
            'http://reganmian.net/blog/blog/2015/09/03/sending-and-receiving-email-with-elixir/',
          categories: [],
          blogtitle: 'Random Stuff that Matters',
          author: 'Stian HÃ¥klev'
        }
      }
    }
  },
  cjid8bdy9000pc6seepguwd82: {
    id: 'cjid8bdy9000pc6seepguwd82',
    li: {
      id: 'cjid8bdy9000ac6selisjzvte',
      liDocument: {
        liType: 'li-rss',
        createdAt: '2018-06-13T14:47:08.385Z',
        createdBy: 'op-rss',
        payload: {
          content:
            '<p>This summer, I ran <a href="https://github.com/houshuang/survey">a Phoenix server</a> that provided interactive content for <a href="https://www.edx.org/course/teaching-technology-inquiry-open-course-university-torontox-inq101x">an EdX MOOC</a>. Given the geographical distribution of students, the server was getting hit 24/7, and I wanted a quick way to get notified about any error messages. Elixir comes with a built-in <a href="http://elixir-lang.org/docs/v1.0/logger/">logging framework</a> that has several levels of logging (debug, info, warn, error). Any processes crashing will emit error logs, and you can also emit them manually from your own code.</p>\n\n<p>The stock logger only comes with a console backend. I ran the server through Ubuntu\'s <code>upstart</code> system with a very simple configuration script, which simply set some environment variables and paths, and then used <code>PORT=4000 MIX_ENV=prod mix phoenix.server</code> (I used Nginx to proxy the local port, and add SSL). I used <code>mosh</code> and <code>tmux</code> to keep a persistent connection, and <code>tail -f</code> to watch the upstart log. On the screenshot below, I had several panes tracking both the access log (using <a href="https://github.com/mneudert/plug_accesslog">plug_accesslog</a>), the Phoenix log, and also a <code>grep</code> on <code>error</code> to see the latest error messages.</p>\n\n<p><img src="/blog/images/2015-08-26-email-notifications-about-errors-in-elixir_-_whole-01.png" style="none"></p>\n\n<p>Due to BEAM (the Erlang virtual machine) and <a href="http://elixir-lang.org/getting-started/mix-otp/supervisor-and-application.html">OTP</a>, Phoenix is remarkably stable. The only time the whole server went down would be because of a serious issue in the datacenter rendering the entire virtual machine inaccessible. Errors were typically bugs that would occur given a certain edge case (for example trying to render the group of a user who had not selected a group yet). They would crash the connection of the user who hit that edge case, but would not propagate up the stack. Thus it would be quite possible for an error to occur and not be noticed, since everything else would continue happily running.</p>\n\n<p>I decided I\'d like to get an email every time an error was triggered. Luckily, the Elixir logger was built from the start as a pluggable system, and there are already a number of possible backends, feeding into <a href="https://github.com/smpallen99/syslog">syslog</a>, or several "monitoring as a service" systems like <a href="https://github.com/honeybadger-io/honeybadger-elixir">Honeybadger</a> and <a href="https://github.com/crashdumpio/fink-elixir">Crashdump.IO</a>.</p>\n\n<p>I had already integrated e-mail into my system, using <a href="https://github.com/crashdumpio/fink-elixir"> Mailman </a> to send email through <a href="https://aws.amazon.com/ses/"> AWS Simple Email Service </a> for notifications, and looking at the examples in Elixir, and the external loggers, writing a custom back-end that sends email notifications turned out to be remarkably simple.</p>\n\n<p>Copying some code from the console backend, I ended up with <a href="https://github.com/houshuang/survey/blob/master/lib/error_mail.ex">this module</a>, with only 50+ lines of code, most of which handles config and setup. The real work is done in these two functions, first a <code>gen_event</code> callback whenever a message with level <code>error</code> is emitted, and which simply dispatches to the log_event function:</p>\n\n<pre><code class="elixir">def handle_event({:error, _gl, {_, msg, ts, md}}, state) do\n  log_event(msg, ts, md, state)\n  {:ok, state}\nend\n</code></pre>\n\n<p>then the log_event function, which uses the <a href="http://elixir-lang.org/docs/v1.0/logger/Logger.Formatter.html">Logger.Formatter</a> module to get the string to mail out, briefly checks against two kinds of errors which I did not want logged by email, and then composes and sends an email, using exactly the same functionality that I use for sending a new user notification.</p>\n\n<pre><code class="elixir">defp log_event(msg, ts, md, {from, to_list, format, metadata}) do\n  msg = Logger.Formatter.format(format, :error, msg, ts, Dict.take(md, metadata))\n        |&gt; IO.iodata_to_binary\n  if !String.contains?(inspect(msg), "GenServer :job_worker") &amp;&amp;\n    !String.contains?(inspect(msg), "bad_charset") do\n    %Mailman.Email{\n      from: from,\n      to: to_list,\n      text: msg}\n    |&gt; Survey.Mailer.deliver\n  end\nend\n</code></pre>\n\n<p>This worked perfectly.</p>\n\n<p><img src="/blog/images/2015-08-26-email-notifications-about-errors-in-elixir_-_whole-02.png" style="none"></p>\n\n<p>A trick when developing something like this, is to set the level to <code>warn</code> while developing. Otherwise, if you have an error in your logging code, emitting an error log message from IEx to test your code, will trigger an error log message about the error in your logger, which will trigger another one, etc. This also means that in general, it is very important to make sure that the error logging code cannot itself trigger errors. If sending the email might cause an error, it might be better for the mailer to be a separate gen_event, and for the logger to simple send a message to it.</p>\n\n<p>There is no checking for duplicate errors in this system, which means that in a situation where a lot of errors are emitted, you will receive a lot of emails.</p>\n\n<p><img src="/blog/images/2015-08-26-email-notifications-about-errors-in-elixir_-_whole-03.png" style="none"></p>\n\n<p>This sometimes happened when I forgot to do a <code>mix deps.get</code> after pushing new code that had an added dependency (I didn\'t use any kind of release management, but simply pushed code to Github, pulled it onto the server, and restarted the server). It once happened that an incoming e-mail triggered a panic with <code>gen_smtp</code> because of a failing charset - this e-mail was resent every ten minutes for 24 hours, which resulted in many error messages. It also happened that the server was rebooted into single-user mode because of a datacenter failure, which meant it could not connect to the database. Upstart will automatically restart the system on a crash, again resulting in a stream of error mails.</p>\n\n<p>I\'m not sure how one could handle this, because I think it makes sense to keep the error logging system as simple and robust as possible. A <code>gen_server</code> which handled e-mailing and kept counters would help to some extent, but not in the case of multiple restarts. In that situation, you would need an external service which aggregated emails and had conditional dispatch, and then you are getting into the level of building up a monitoring and reporting infrastructure, either internally or as a service.</p>\n\n<p>However, this was a great first try - it did work very reliably, and it\'s also helpful to see that there is very little magic to <code>Logger</code> and the logger backends.</p>',
          title: 'Email notifications about errors in Elixir',
          date: '2015-08-26T16:41:48.000Z',
          link:
            'http://reganmian.net/blog/blog/2015/08/26/email-notifications-about-errors-in-elixir/',
          categories: [],
          blogtitle: 'Random Stuff that Matters',
          author: 'Stian HÃ¥klev'
        }
      }
    }
  },
  cjid8bdy9000qc6se9bx4wop6: {
    id: 'cjid8bdy9000qc6se9bx4wop6',
    li: {
      id: 'cjid8bdy9000bc6sel5npe57l',
      liDocument: {
        liType: 'li-rss',
        createdAt: '2018-06-13T14:47:08.385Z',
        createdBy: 'op-rss',
        payload: {
          content:
            '<h2>Introduction</h2>\n\n<p>I\'ve been spending the summer in China, head-deep in <a href="http://elixir-lang.org/">Elixir</a>, writing interactive scripts for an <a href="https://www.edx.org/course/teaching-technology-inquiry-open-course-university-torontox-inq101x">EdX MOOC on inquiry and technology for teachers</a> (<a href="https://imgur.com/a/rAXVz">screenshots</a>). I will have much more to share about that project later, but now I wanted to share a small Elixir package I released on Github called <a href="https://github.com/houshuang/elixir-prelude">Prelude</a> (<a href="https://houshuang.github.io/prelude">documentation</a>). This is a collection of utility functions for Elixir that I\'ve extracted out of my code base.</p>\n\n<h2>Organization</h2>\n\n<p>I began by scattering these utility functions all over as small <code>defp</code>s, but when I wanted to reuse them in other modules, I began gathering them. Initially I had a single file called Prelude - modelled after the Haskell Prelude, just because it was the only file I\'d ever wholesale <code>import</code> into my other modules.</p>\n\n<p>Eventually I began separating the code out into sub-modules, which meant I went from</p>\n\n<pre><code class="elixir">import Prelude\n\nmap_atomify(...)\n</code></pre>\n\n<p>to</p>\n\n<pre><code class="elixir">Prelude.Map.atomify(...)\n</code></pre>\n\n<p>which seems cleaner.</p>\n\n<h2>The point of gathering "trivial" functions</h2>\n\n<p>Many of these functions are very simple, but yet things you use all the time - gathering them together both means less repetition and greater legibility. Compare:</p>\n\n<pre><code class="elixir">x\n|&gt; Enum.map(fn {k, v} -&gt; {v, k} end)\n|&gt; Enum.into(%{})\n</code></pre>\n\n<p>to</p>\n\n<pre><code class="elixir">x\n|&gt; Prelude.Map.switch\n</code></pre>\n\n<p>The code in the first version is short enough that I wouldn\'t bother making it a separate function, but the intent of the second snippet is much clearer. I might also forget to do <code>Enum.into(%{})</code>, and somehow I always forget the <code>end</code> in anonymous functions. So, some quick time saving.</p>\n\n<h2>Map functions</h2>\n\n<p>Some of the functions are a bit more involved, mainly found in the <a href="http://houshuang.github.io/prelude/Prelude.Map.html">Prelude.Map</a> module. I was inspired by the way Clojurists often work with deeply nested maps, sometimes keeping the entire application state in one <code>atom</code>. This has also become relevant in Javascript-land with experiments around immutable state such as <a href="https://github.com/Yomguithereal/baobab">Baobab</a>.</p>\n\n<p>I see deeply nested maps much less often in Elixir. Part of it might be the architecture (we use gen_servers, message passing, and ETS tables, rather than atom-swapping), but part of it might also be because of the syntax (<code>%{%{%{}}}</code> is not pretty).</p>\n\n<p>However, Elixir already has <code>get_in</code> and <code>put_in</code> to reach into deeply nested objects. Prelude provides <code>deep_put</code>, which is a parallel to <code>mkdir -p</code>, it will insert an object and create any part of the missing path. If you put to a path that already exists, it will turn it into an array.</p>\n\n<p>Built on top of <code>deep_put</code> is <code>group_by</code>. Building the Phoenix app, I often had to extract data from the database using Ecto, and render it in templates. Let\'s say you have a table of students, listing their group affiliation, and which class their in. You wish to list all students, sorted by class, and then by group. With <code>group_by</code>, you can get the data from Ecto like this:</p>\n\n<pre><code class="elixir">MyRepo.all(Student)\n|&gt; Prelude.Map.group_by([:class, :group])\n</code></pre>\n\n<p>The result will be a map of classes. Each class will point to a map of groups, and each group will point to a list of students in that group. Then in an EEx template, you can display the list like this:</p>\n\n<pre><code class="elixir">&lt;%= for class &lt;- @classes %&gt;\n  &lt;h2&gt;&lt;%= class %&gt;&lt;/h2&gt;\n  &lt;%= for group &lt;- class %&gt;\n    &lt;h3&gt;&lt;%= group %&gt;&lt;/hr&gt;\n    &lt;%= for student &lt;- group %&gt;\n      &lt;li&gt;&lt;%= student %&gt;&lt;/li&gt;\n    &lt;% end %&gt;\n  &lt;% end %&gt;\n&lt;% end %&gt;\n</code></pre>\n\n<h2>Wrapping up</h2>\n\n<p>Anyway, the code is available, so feel free to browse around. I will be using this in my future projects, and probably adding functionality that is general enough to stand on its own. I\'d love to add tests, and am happy to think about naming, APIs etc, if anyone else think this is interesting.</p>\n\n<p>It\'s also my first time to use <a href="https://github.com/elixir-lang/ex_doc">ExDoc</a> and <a href="https://pages.github.com/">Github Pages</a>, but both were a breeze.</p>',
          title: 'Elixir Prelude: Packing up utility functions',
          date: '2015-08-21T16:41:48.000Z',
          link:
            'http://reganmian.net/blog/blog/2015/08/21/elixir-prelude-packaging-up-utility-functions/',
          categories: [],
          blogtitle: 'Random Stuff that Matters',
          author: 'Stian HÃ¥klev'
        }
      }
    }
  },
  cjid8bdy9000rc6sevyfm7mx8: {
    id: 'cjid8bdy9000rc6sevyfm7mx8',
    li: {
      id: 'cjid8bdy9000cc6sevrfnsbpv',
      liDocument: {
        liType: 'li-rss',
        createdAt: '2018-06-13T14:47:08.385Z',
        createdBy: 'op-rss',
        payload: {
          content:
            '<p>I always enjoy finding new ways of automating boring tasks, and today I had a few different small tasks that I was able to solve using some terminal-fu, and Vim macros. I really enjoyed how well it came together, so I thought I\'d put together a short screencast to demonstrate.</p>\n\n<p>I had generated a number of discussion forum graphs for different courses. The processed data is in a json file in a directory named after the course. My task was to put an identical index.html file into each directory, create an index of the courses, with links, and modify all the landing pages to reflect the names of the courses. Not a hugely complicated task, but something you could run into day to day, and which would be quite cumbersome to do manually.</p>\n\n<iframe width="600" height="500" src="https://www.youtube.com/embed/HesocMwPuJc" frameborder="0" allowfullscreen></iframe>\n\n<p>The video should be quite self-explanatory, but I\'ve included some "show-notes" below.</p>\n\n<p>As far as I know, <code>ls</code> doesn\'t have a built-in way to list only directories, so I found this snippet somewhere and added it to my .zshrc:</p>\n\n<pre><code class="bash">alias ld="ls -lht | grep \'^d\'"\n</code></pre>\n\n<p><code>awk</code> is a very powerful string manipulation tool, but in this case we\'re just using it to get the 9th column of each line (<code>awk \'{ print $9 }\'</code>)</p>\n\n<p><code>xargs</code> is very useful to run commands on input arguments. We need <code>-n 1</code>, otherwise it would put use all the arguments in the same command. With <code>-n 1</code>, it executes the command once for each input argument. If we didn\'t want to put the arguments last, we could do something like <code>echo 1 2 3 | xargs -n 1 - J % echo % hi</code>.</p>',
          title: 'Fun with terminal and Vim macros',
          date: '2015-04-19T16:41:48.000Z',
          link:
            'http://reganmian.net/blog/blog/2015/05/19/fun-with-terminal-and-vim-macros/',
          categories: [],
          blogtitle: 'Random Stuff that Matters',
          author: 'Stian HÃ¥klev'
        }
      }
    }
  },
  cjid8bdy9000sc6se0soy2tw8: {
    id: 'cjid8bdy9000sc6se0soy2tw8',
    li: {
      id: 'cjid8bdy9000dc6secfx9bci5',
      liDocument: {
        liType: 'li-rss',
        createdAt: '2018-06-13T14:47:08.385Z',
        createdBy: 'op-rss',
        payload: {
          content:
            '<div style="float:right;font-size:80%;margin:30px;"><ul style="margin-top: 0em;margin-bottom: 1.5em;"><li><a href=#0>Installation</a></li><li><a href=#1>Cohorts and single-sign on</a></li><li><a href=#2>LTI to the rescue</a></li><li><a href=#3>Early experiment, group-based Etherpad selection</a></li><li><a href=#4>Confluence integration and templates</a></li><li><a href=#5>See it in action</a></li></ul></ul></div><p><strong>I came up with a neat way to embed external web tools into EdX courses using an LTI interstitial, which also allows for rich pedagogical scripting and group formation. Illustrated with <a href="https://www.youtube.com/watch?v=-OY9UPT4dK8">a brief screencast</a>.</strong></p>\n\n<p>Last year, I <a href="http://reganmian.net/blog/2014/10/03/a-pedagogical-script-for-idea-convergence-through-tagging-etherpad-content/">experimented</a> with <a href="http://reganmian.net/blog/2014/10/03/supporting-idea-convergence-through-pedagogical-scripts/">implementing pedagogical scripts</a> spanning different collaborative Web 2.0 tools, like Etherpad and Confluence wiki, in two hybrid university courses.</p>\n\n<p><a href="http://reganmian.net/blog/2014/10/03/supporting-idea-convergence-through-pedagogical-scripts/"><img src="http://reganmian.net/blog/images/2014-10-03-a-pedagogical-script-for-idea-convergence-through-tagging-etherpad-content_-_whole-04.png" style="none"></a></p>\n\n<p>Coordinating collaboration and group formation in a class of 85 students was already challenging, but currently we are planning a MOOC built on the same principles. Before we begin to design the pedagogical scripts and the flow of the course, a key question was how much we could possible implement technically given the MOOC platform. Happily, it turns out that it will be easier and more seamless than I had feared.</p>\n\n<p>We chose to go with EdX (University of Toronto has agreements with both EdX and Coursera), which is based on an open-source platform. Open source can mean many different things, there are platforms that are mainly developed privately in a company, and released to the public as periodic "code dumps", and where installation is very complex, and the product is unlikely to be used by anyone else. I didn\'t know much about the EdX code, but the fact that the platform is already used by <a href="https://github.com/edx/edx-platform/wiki/Sites-powered-by-Open-edX">a very impressive number of other installations</a>, such as China\'s <a href="https://www.xuetangx.com/">XuetangX</a> and <a href="https://www.france-universite-numerique-mooc.fr/cours/">France\'s UniversitÃ© NumÃ©rique</a> was already a very positive sign. </p>\n\n<!-- more -->\n\n<h1 id=0>Installation</h1>\n\n<p><img src="/blog/images/2015-02-10-openedx-and-lti-pedagogical-scripts-and-sso_-_half-01.png" style="float:right; margin-left: 10px">\nThe OpenEdX code is available on GitHub, and seems to be a very active community -- more than 28k commits by 199 contributors, &gt;100 pull requests, 621 branches... But the most important thing is that it is very easy to install. I remember when we were building P2PU\'s Lernanta platform, also on Django, and it was quite difficult to install the code locally (having the right Python version, pulling in all the right libraries, etc), even to make some tiny changes. OpenEdX, which is much more complex, with a number of different services that need to work together, offers several pre-built packages. I chose to use Vagrant to run it locally, and once you\'ve installed Vagrant and VirtualBox, simply using the commands listed <a href="https://github.com/edx/configuration/wiki/edx-Full-stack--installation-using-Vagrant-Virtualbox">here</a> was enough to get a fully functioning local OpenEdX instance:</p>\n\n<pre><code class="bash">mkdir fullstack\ncd fullstack\ncurl -L https://raw.githubusercontent.com/edx/configuration/master/vagrant/release/fullstack/Vagrantfile &gt; Vagrantfile\nvagrant plugin install vagrant-hostsupdater\nvagrant up\n</code></pre>\n\n<h1 id=1>Cohorts and single-sign on</h1>\n\n<p>Now that I had a functional version of OpenEdX running locally, it was time to look at our challenges. We were interested in two separate but connected pieces of functionality. </p>\n\n<ul>\n<li>enabling single-sign on with external tools, so that students can use an external wiki, for example, without having to create a new account and remember a new password</li>\n<li>ways of "scripting" the collaboration process, particularly grouping students into smaller groups, and sending students to various resources based on time, group affiliation, and what they had already done</li>\n</ul>\n\n<p><img src="/blog/images/2015-02-10-openedx-and-lti-pedagogical-scripts-and-sso_-_half-02.png" style="float:right; margin-left: 10px">\nFor the second challenge, EdX already has a "cohort-feature", which let\'s you divide students into groups, and make certain parts of the discussion forum specific to each group. However, we were interested in much more detailed scripts than that. In our existing course, we used APIs to automatically insert templates into each group\'s space, take content from different groups and aggregate together, conditionally send students to different tools depending on their previous input, etc. Since EdX does not have an API that gives an instructor live access to the discussion forum, we couldn\'t for example create 300 cohort-groups and automatically populate each forum thread with a question post, then extract their answers, and do something with them. We would thus need to use external tools for this aspect of the course, bringing us back to the single-sign on issue.</p>\n\n<p><img src="/blog/images/2015-02-10-openedx-and-lti-pedagogical-scripts-and-sso_-_half-04.png" style="float:right; margin-left: 10px">\nInspired in particular by the recent EdX MOOC <a href="https://www.edx.org/course/data-analytics-learning-utarlingtonx-link5-10x">Data, Analytics and Learning</a>, and their inclusion of external social and collaborative tools, I began investigating our options. The simplest way of including external tools, beyond simply linking to them, is to use an IFrame. However, a tool included in an IFrame receives no information from EdX about which student is visiting, and can send no information back.</p>\n\n<p>An intriguing option that the <a href="https://linkresearchlab.org/dalmooc/">DALMOOC</a> used, is to let the EdX platform function as an OpenID provider, and having a "Login with EdX" button, similar to the many sites that feature "Login with Google/Facebook/Twitter, etc". However, I could not find any information about how to enable this on EdX.org, and it would also mean that the students still would have to do a round-trip back to EdX to allow the login, making it less seamless.</p>\n\n<h1 id=2>LTI to the rescue</h1>\n\n<p><img src="/blog/images/2015-02-10-openedx-and-lti-pedagogical-scripts-and-sso_-_half-03.png" style="float:right; margin-left: 10px">\nAlthough I\'ve been working with hybrid and online learning for many years, I\'ve always been drawn to existing Web 2.0 tools, using APIs and RSS feeds to tie things together. Because of this, I don\'t have much experience with traditional LMSes, and things like SCORM and LTI -- in fact, they always sounded quite cumbersome and complex. However, it turns out that LTI (<a href="http://www.imsglobal.org/toolsinteroperability2.cfm">Learning Tools Interoperability</a>) is a very simple protocol; it\'s basically a simple web request (either as an IFrame, or as a link opening a separate window) with three extra pieces: </p>\n\n<ul>\n<li>\n<strong>Authentication</strong><br>\nThe authentication ensures that the request really comes from the OpenEdX platform (in this case), and that I cannot easily fake a request with someone else\'s user ID.</li>\n<li>\n<strong>Student info</strong><br>\nThe student info in the case of EdX is not any actual student info, but a persistent hashed identifier. This means I can always recognize when the same student accesses the LTI object, and I can also later connect learning analytics from the LTI object with EdX data (there is a hash table connecting the hash IDs with the actual student IDs in the data dumps).</li>\n<li>\n<strong>Callback URL</strong><br>\n The callback URL is for graded LTI objects, to deliver a simple numeric grade back to the EdX platform.<br>\n</li>\n</ul>\n\n<p>I first spent some time researching an LTI integration directly with <a href="http://confluence.atlassian.com">Confluence</a>, but I could not find one that was currently maintained. Limiting ourselves only to web apps that offer LTI integration would also severly restrict our choices, and even if the tools did offer LTI integration, that would only give us something like single-sign on, not any opportunity to implement pedagogical scripts. This led to idea of using an interstitial LTI script. This script would receive the request from OpenEdX, keep track of the relevant information about the student ID, and forward to various web services, providing automatic logon or deep-linking where appropriate/possible.</p>\n\n<h1 id=3>Early experiment, group-based Etherpad selection</h1>\n\n<p>I found <a href="https://github.com/instructure/ims-lti.git">an LTI library for Ruby</a>, which had a great little <a href="https://github.com/instructure/lti_tool_provider_example">minimal example application</a>. The application is simple -- it prompts the user for a grade, and then sends that grade back to the platform -- but the fact that I could get that to work with OpenEdX using <a href="http://edx-partner-course-staff.readthedocs.org/en/latest/exercises_tools/lti_component.html">the provided instructions</a>, gave me the confidence to keep experimenting. I decided to use <a href="http://redis.io/">Redis</a> as a backing database, since it is very easy to experiment with, and would also provide easy scaling if needed in the future. </p>\n\n<p><img src="/blog/images/2015-02-10-openedx-and-lti-pedagogical-scripts-and-sso_-_half-05.png" style="float:right; margin-left: 10px">Since we don\'t know anything about the student arriving, other than the hashed persistent id (there might be ways of getting more information from the system in the future), my first step was a simple form where students would specify their nickname, and choose a group (this could in the future be a long list of groups, or even some complex group formation algorithm).</p>\n\n<p><img src="/blog/images/2015-02-10-openedx-and-lti-pedagogical-scripts-and-sso_-_whole-01.png" style="none">\nThe next access point then simply forwards to an Etherpad, however it chooses the Etherpad URL based on the group already select (and stored in Redis). This way, we could have three hundred groups, and funnel students into 300 different Etherpads without changing anything in the EdX course setup. We could even have an on-demand feature which puts entrants into a "waiting room" until a pre-determined number of students appear, upon which a new Etherpad room is generated, students are forwarded to that room, and a new waiting room is created. </p>\n\n<h1 id=4>Confluence integration and templates</h1>\n\n<p>Since Etherpad does not use any authentication, it was a much simpler thing to integrate than Confluence. Luckily, Confluence has a rich API, which I\'ve already used extensively in previous courses, however I did not know if it would be possible to automatically log students in. In the end, it turns out that by accessing a particular URL, you can offer as arguments the username, password, and the page to be redirected to:</p>\n\n<pre><code class="ruby">url = "https://www.WIKIURL.org/dologin.action?os_username=#{wiki_username}" + \n"&amp;os_password=#{wiki_pwd}&amp;os_destination=/display/IN/#{page}".\n</code></pre>\n\n<p>When a user tries to access a wiki endpoint, I first check in Redis whether the user already has a wiki password. If not, I randomly generate a wiki username and password, and create that user with the appropriate permissions, using the Confluence API. I then redirect the user\'s browser to the URL above, with the username and password (which they never need to see). The result is that they are seemlessly logged in, and redirected to the page I want.</p>\n\n<p><img src="/blog/images/2015-02-10-openedx-and-lti-pedagogical-scripts-and-sso_-_half-07.png" style="float:right; margin-left: 10px">\nHowever, having thousands of users simultaneously edit an unstructured wiki can be chaotic. Our idea is to make it easy for students to supply information, and for a smaller group of engaged users to engage with editing and enriching that information. As an example, I created a simple form which then populates a wiki page. In the current example, upon visiting this LTI object, the student automatically gets a wiki account (if he/she doesn\'t already have one), the page is created, and the student is automatically logged in and sent to the newly created page. In an actual course, the page could be generated by information submitted by numerous students, and accessing the page wouldn\'t have to follow immediately after filling out the template.</p>\n\n<p><img src="/blog/images/2015-02-10-openedx-and-lti-pedagogical-scripts-and-sso_-_whole-02.png" style="none"></p>\n\n<h1 id=5>See it in action</h1>\n\n<iframe width="420" height="315" src="https://www.youtube.com/embed/-OY9UPT4dK8" frameborder="0" allowfullscreen></iframe>\n\n<p>Although this example is quite simple, I find it very promising. I am very happy that due to the OpenEdX platform being so easy to install, and the Ruby LTI library comes with an example app, I was able to get this running in an afternoon. Now comes the hard work -- designing meaningful scripts and interaction patterns for students with very different reasons to participate, and levels of engagement, but at least we know that we can probably implement what we want to. We also need to think about scaling, and whether our local services can handle the amount of users we are expecting.</p>\n\n<p>I\'d love to hear about others experimenting with pushing on the interactive and collaborative features in MOOCs and large courses. It would also be interesting to think about how to make LTI tools even easier to integrate -- I\'d love to see a catalogue of external LTI tools, ranging from simple individual widgets, to groupware and collaboration tools, which could all be easily slotted into an EdX, Instructure, or even Blackboard course.</p>\n\n<p>All the example code <a href="https://github.com/houshuang/inquirymooc">is on GitHub</a>.</p>',
          title: 'OpenEdX and LTI: Pedagogical scripts and SSO',
          date: '2015-02-20T16:41:48.000Z',
          link:
            'http://reganmian.net/blog/blog/2015/02/20/openedx-and-lti-pedagogical-scripts-and-sso/',
          categories: [],
          blogtitle: 'Random Stuff that Matters',
          author: 'Stian HÃ¥klev'
        }
      }
    }
  },
  cjid8bdy9000tc6se1zv3bdb1: {
    id: 'cjid8bdy9000tc6se1zv3bdb1',
    li: {
      id: 'cjid8bdy9000ec6se333yyl9l',
      liDocument: {
        liType: 'li-rss',
        createdAt: '2018-06-13T14:47:08.385Z',
        createdBy: 'op-rss',
        payload: {
          content:
            '<div style="float:right;font-size:80%;margin:30px;"><ul style="margin-top: 0em;margin-bottom: 1.5em;"><li><a href=#0>Why introduce German content in English?</a></li><li><a href=#1>Podcasts and radio</a></li><ul style="margin-top: 0em;margin-bottom: 1.5em;"><li><a href=#2>Radio programs I like</a></li><li><a href=#3>Podcasts I like</a></li></ul><li><a href=#4>TV, movies and series</a></li><li><a href=#5>Literature</a></li></ul></div><h1 id=0>Why introduce German content in English?</h1>\n\n<p>One issue that I have not heard much discussion about, is the challenge of finding interesting material in foreign languages. In your own culture and language, you have gotten to know authors through school, through popular media, recommendations from friends, etc. But when you begin exploring a foreign language, it\'s often like starting from scratch again. I remember walking around in the libraries in Italy, having no idea where to start, or what kinds of authors I might enjoy. </p>\n\n<p>This is made worse by the fact that I am typically much slower at reading in a foreign language, and particularly in "skimming". This means that even though I can really enjoy sitting down with a nice novel, and slowly working through it, actually keeping up with a bunch of blogs, skimming the newspaper every day, or even doing a Google-search and flitting from one page to the next, becomes much more difficult. </p>\n\n<p>Some things are easier these days - it\'s typically easier to get access to media over the Internet, whether it be movies, podcasts, or ebooks (although this differs a lot between different languages, it is still difficult finding Indonesian movies or TV-shows online, and even Norwegian e-books are hard to come by online), but navigating is still difficult. When I began learning Russian again, I was amazed at the number of very high-quality English blogs that talk about Russian modern literature (places like <a href="http://lizoksbooks.blogspot.ca/">Lizok\'s bookshelf</a>, <a href="http://xixvek.wordpress.com/">XIX Ð²ÐµÐº</a> and <a href="http://languagehat.com/">Languagehat</a>), which really inspired me to continue working on my Russian, to be able to read the books they so enthusiastically discussed.</p>\n\n<p><img src="/blog/images/2014-10-17-random-german-stuff-that-matters---great-content-auf-deutsch_-_whole-08.png" style="float:right; margin-left: 10px">\nThese last two years have been a bit of a renaissance for my own usage of German. It\'s a language I learnt in school in Norway, I read <a href="http://reganmian.net/blog/2008/07/19/celebrating-first-book-read-in-a-foreign-language/">my first novel</a> in it when I was 16, and have visited the country a number of times. However, for the past 10-15 years, I did not have many opportunities to use it, and did not seek out German novels, podcasts or anything like that. However, spending a very enjoyable month visiting a lab at the University of Munich gave me a chance to reawaken my language skills and interest. I then did a short visit last winter, and spent about three months in Berlin this summer. </p>\n\n<p>In addition to the visits, what got me in to reading was ironically Karl May. Not well known in North America, he is the all-time best-selling German author from the 19th Century, writing a large amount of fantastical tales of travel and adventure around the world, from places he had never visited. I had heard about him, but never read anything, and decided that I would try it out for fun. I found <a href="http://www.karl-may-gesellschaft.de/kmg/primlit/roman/herzen/index.htm">"Deutsche Herzen, deutsche Helden"</a> as a free download and began. It was certainly a fantastic tale, taking us from Istanbul, to Egypt, through the Wild West and ending up in Siberia. </p>\n\n<p>Long like a Bollywood-movie, it felt like each part could have been it\'s own substantial book. Full of handsome and brave Germans, it was certainly not always politically correct (although I have read far worse things in English from the colonial times), but quite enjoyable, and at the end, I realized I had read almost 3000 pages (these things happen on a Kindle), and that my German reading speed had improved measurably as a result.</p>\n\n<p>Since then, I have found a great number of enjoyable German novels and podcasts, and I wanted to share some of these with you. If you speak German as a foreign language, but were not quite sure where to start, perhaps some of this will be useful. And if you don\'t speak German, perhaps you\'ll be inspired to learn. Of course, this is just a sampling of what I\'ve randomly come across, not an exhaustive survey, and it unapologetically is stuff that I like and enjoy, no attempt at objective critique.</p>\n\n<h1 id=1>Podcasts and radio</h1>\n\n<p>I love going for walks and listening to podcasts and audio books. I\'ve also found that audio supports language learning and maintenance in a different way than reading; in the same way that reading a lot can make you a better writer, I find myself speaking more fluently and confidently after listening to audio in a language, even if I haven\'t actually spoken it for a long time. A good example was when I met someone speaking German in a Moscow airport a few years ago. At that time, I hadn\'t been speaking  German for years, but just finished listening to <a href="https://en.wikipedia.org/wiki/Momo_(novel)">Momo</a>, and was surprised by my fluency.</p>\n\n<p>Germany has a very vibrant private podcasting scene, and also very high-quality TV and radio stations. Most of these have so-called Mediathek, where you can easily find programs and links to feeds (like <a href="http://www.ardmediathek.de/tv">ARD\'s Mediathek</a>).</p>\n\n<h1 id=2>Radio programs I like</h1>\n\n<p><strong>DRadio Wissen Einhundert</strong></p>\n\n<p><a href="http://dradiowissen.de/einhundert"><img src="/blog/images/2014-10-17-random-german-stuff-that-matters---great-content-auf-deutsch_-_whole-01.png" style="none"></a></p>\n\n<p><strong><a href="http://dradiowissen.de/einhundert">Einhundert</a></strong> is my favorite German radio program. Every week, they choose a theme, which can be very broad ("Starting over", "Hitting the road"), but also more specific (a program about when half a million Russian soldiers left Eastern Germany). In each case, they find people with interesting stories that somehow relate to the theme (often in very different ways).</p>\n\n<p>It\'s a bit similar to <a href="http://www.thisamericanlife.org/">This American Life</a> or <a href="http://www.radiolab.org/">RadioLab</a> (the latter of which I love as well), but they have managed to find their own voice. It is difficult to put the finger on what is so brilliant about this program, but somehow the combination of interesting stories and good editing makes for great listening. I often tell my wife about stories I head in this podcast, two of the more memorable ones:</p>\n\n<ul>\n<li><a href="http://dradiowissen.de/beitrag/wie-aus-papa-abui-wurde">A son learns Arabic as an adult, and begins to understand his father in a whole new way</a></li>\n<li><a href="http://dradiowissen.de/beitrag/iran-die-stellvertreter-reise-in-meine-heimat">A girl has never been to her homeland, Iran, and is not allowed to travel there. Her boyfriend goes in her place, as her eyes and ears</a></li>\n</ul>\n\n<p><strong>News</strong></p>\n\n<p><img src="/blog/images/2014-10-17-random-german-stuff-that-matters---great-content-auf-deutsch_-_half-01.png" style="float:right; margin-left: 10px">You can watch or download the video of the main evening news in Germany, <strong><a href="http://www.tagesschau.de/">Die Tagesschau</a></strong>, and sometimes I watched the news on my iPad while doing dishes. However, my preference is to download the audio and listen while walking. Surprisingly, I find I never really miss the visuals. I appreciate the focus on domestic and international politics, rather than "a man was hit by a bus yesterday".</p>\n\n<p>Listening regularly, you get a good understanding of the main political issues in Germany and the EU, and how they are developing. It\'s been particularly interesting following the Ukraine-conflict through German eyes. I also often listen to <strong><a href="http://www.tagesschau.de/sendung/tagesthemen/">Tagesthemen</a></strong>, which tends to go a bit more in depth. There is also <strong><a href="http://www.tagesschau.de/bab/">Bericht aus Berlin</a></strong>, which is a weekly political magazine.</p>\n\n<p><strong>Other academic programs</strong></p>\n\n<p><a href="http://dradiowissen.de/beitrag/musik-digitale-reproduzierbarkeit-beat-mp3"><img src="/blog/images/2014-10-17-random-german-stuff-that-matters---great-content-auf-deutsch_-_half-02.png" style="float:right; margin-left: 10px"></a>\n<strong><a href="http://dradiowissen.de/hoersaal">DRadio HÃ¶rsaal</a></strong> is an interesting format, where they first interview a researcher about their research, followed by a public lecture made by the same person. The topics vary widely, and I remember hearing <a href="http://dradiowissen.de/beitrag/musik-digitale-reproduzierbarkeit-beat-mp3">a fascinating talk</a> about how we now play classical music faster, than when it was authored.</p>\n\n<p><strong><a href="http://www.swr.de/swr2/wissen/">SWR Wissen</a></strong> is a series of traditional radio documentaries, where a journalist tells a story, intermixed with some interviews. Topics vary very widely, but somehow the good story-tellers are able to make many topics that don\'t sound very enticing into interesting experiences. (Looking at the site now, I see that they also make <a href="http://www.swr.de/swr2/wissen/epub/-/id=661224/nid=661224/did=9072786/1w7y7sx/index.html">transcripts of all programs available in epub-format</a> - quite innovative.)</p>\n\n<h1 id=3>Podcasts I like</h1>\n\n<p><strong>Chaosradio</strong></p>\n\n<p><a href="http://www.ccc.de/en/"><img src="/blog/images/2014-10-17-random-german-stuff-that-matters---great-content-auf-deutsch_-_whole-02.png" style="float:right; margin-left: 10px"></a>\n<a href="http://www.ccc.de/en/">Chaos Computer Club</a> is a historically famous network of hacker-clubs in Germany, and their Berlin chapter produce a monthly radio program/podcast called <strong><a href="http://chaosradio.ccc.de/chaosradio.html">Chaosradio</a></strong>. The goal is to explain technical concepts to a more lay audience, so they usually have members of the club moderated by a radio moderator. The shows last something like three hours, and it can be quite enjoyable to hear the friendly bantering of the participants. For example, I quite enjoyed the show last winter <a href="http://chaosradio.ccc.de/cr197.html">about the NSA surveillance scandal</a> and how mass-digital surveillance works.</p>\n\n<p><strong>CRE: Technik, Kultur, Gesellschaft</strong></p>\n\n<p><a href="http://cre.fm"><img src="/blog/images/2014-10-17-random-german-stuff-that-matters---great-content-auf-deutsch_-_half-03.png" style="float:right; margin-left: 10px"></a>\nThrough Chaosradio, I came across <strong><a href="http://cre.fm/">CRE</a></strong>, which began as an extension to Chaosradio, but now has a life of its own. It\'s run by <a href="http://metaebene.me/timpritlove/">Tim Pritlove</a>, who is an incredibly prolific German podcaster, and the format is basically Tim sitting down with someone who is a specialist on a certain topic, and just talking. Sometimes the conversations will go over 3-4 hours, and somehow listening to two very intelligent people in a nice conversation can be quite interesting and calming. Some of my favorite episodes:</p>\n\n<ul>\n<li>\n<a href="http://cre.fm/cre205-wikidata">Wikidata</a>, the new semantic Wikimedia project, and how we think about ontologies and hierarchies to organize knowledge</li>\n<li>\n<a href="http://cre.fm/cre200-stadtplanung">Stadtplanung</a>, city planning from Alexandria and Babylon to Russian suburbs, Syria and Berlin</li>\n<li>\n<a href="http://cre.fm/cre184-bundeswehr">Bundeswehr</a>, the role of the German army in international conflicts</li>\n</ul>\n\n<p><a href="http://fokus-europa.de/"><img src="/blog/images/2014-10-17-random-german-stuff-that-matters---great-content-auf-deutsch_-_half-05.png" style="float:right; margin-left: 10px"></a>\nTim also hosts a few other noteworthy podcasts, including <strong><a href="http://logbuch-netzpolitik.de/">Logbuch:Netzpolitik</a></strong>, which reminds me of many English-language podcasts, where two people sit around every week and chat about "what\'s new" related to Internet and technology politics. A more structured interview-podcast is <strong><a href="http://fokus-europa.de/">Fokus Europa</a></strong> which attempts to explain the structure of modern Europe, focused mostly on the European Union. I found the first two episodes <a href="http://fokus-europa.de/podcast/fe001-geschichte-der-europaeischen-einigung/">Geschichte der EuropÃ¤ischen Einigung</a> and <a href="http://fokus-europa.de/podcast/fe002-deutschland-und-europa/">Deutschland und Europa</a> quite interesting.</p>\n\n<p><a href="http://www.staatsbuergerkunde-podcast.de"><img src="/blog/images/2014-10-17-random-german-stuff-that-matters---great-content-auf-deutsch_-_half-04.png" style="float:right; margin-left: 10px"></a>\n<strong><a href="http://www.staatsbuergerkunde-podcast.de">StaatsbÃ¼rgerkunde</a></strong> is a great example of oral history. Martin Lutz interviews his parents Martin and Christine Lutz about their life in the DDR, mixing general knowledge and history with their own personal recollections and anecdotes. I particularly enjoyed the episode on <a href="http://www.staatsbuergerkunde-podcast.de/2013/11/23/sbk030-weisensee/">Weissensee</a>, the drama series I mention below.</p>\n\n<h1 id=4>TV, movies and series</h1>\n\n<p>This will unfortunately be a very short section, and I would love further suggestions. I enjoy watching German movies when I come across them, but I have no good online source to find them (Netflix only has a few). I have also been looking for good German drama series, and would love to find something like the great Danish crime dramas (like <a href="https://en.wikipedia.org/wiki/The_Bridge_(Danish/Swedish_TV_series)">The Bridge</a>). However, I\'ll mention one great drama series and a serie of documentaries.</p>\n\n<p><a href="https://de.wikipedia.org/wiki/Terra_X:_Deutschland_von_oben"><img src="/blog/images/2014-10-17-random-german-stuff-that-matters---great-content-auf-deutsch_-_whole-03.png" style="none"></a></p>\n\n<p><strong><a href="https://de.wikipedia.org/wiki/Terra_X:_Deutschland_von_oben">Deutschland von oben</a></strong> is a series of documentaries using a combination of satellite and aerial footage to show different aspects of the geography of Germany. There are several themes, "City", "Land" and "Water", and we go from looking at the majestic golden eagles in Berchtesgaden, to how current street patterns are the result of the structure of the initial Roman settlements. It gave me a much better understanding of the diversity of German landscapes and regions, and although it can get tiring, the dramatic music and voices go well with the sweeping vistas.\n<a href="http://www.daserste.de/unterhaltung/serie/weissensee/index.html"><img src="/blog/images/2014-10-17-random-german-stuff-that-matters---great-content-auf-deutsch_-_half-06.png" style="float:right; margin-left: 10px"></a>\nAll the episodes are available in streaming HD, and there is also a feature-length cinema version.</p>\n\n<p><strong><a href="http://www.daserste.de/unterhaltung/serie/weissensee/index.html">Weissensee</a></strong> is a mini-series in two seasons about life in East-Berlin during the DDR-times. The series focus on two families, a Stasi-family and a rebellious artist family, whose lives interact in various ways. There are definitively aspects of Romeo and Juliette, and cliches from other mini-series, but it\'s very well done, and apparently very period-authentic. I very much enjoyed both seasons, and also <a href="(http://www.staatsbuergerkunde-podcast.de/2013/11/23/sbk030-weisensee/)">the discussion about the series</a> on the StaatsbÃ¼rgerkunde podcast mentioned above.</p>\n\n<h1 id=5>Literature</h1>\n\n<p>Reading fiction is one of my main ways of approaching a foreign language, and German is a very welcoming host. It has become much easier now that we have access to e-books, and literature portals. I remember when I began learning German and Italian 15 years ago, I would go into a library and be completely lost as to where to start. These days, we have great sites like <a href="http://www.krimi-couch.de/">Krimi-couch</a>, or even <a href="http://www.zeit.de/kultur/literatur/">the literature section in Die Zeit</a> for more high-brow literature.</p>\n\n<p><img src="/blog/images/2014-10-17-random-german-stuff-that-matters---great-content-auf-deutsch_-_half-07.png" style="float:right; margin-left: 10px">\nIn general I quite enjoy crime novels, and I found a German crime writer tradition that felt quite similar to the <a href="http://www.scandinaviancrimefiction.com/">Scandinavian crime</a> that I grew up with. It tends to be tightly bound to a specific region or town, and to spend just as much time on the personal lives of the investigators, as well as the perpetrators and victims of the crime, sometimes also touching upon societal trends and challenges. This is not the hard-boiled crime noir from smokey San Francisco PI-offices, but rather a great way to examine different tensions in society, different occupational groups, life in small and big cities, etc.</p>\n\n<p><img src="/blog/images/2014-10-17-random-german-stuff-that-matters---great-content-auf-deutsch_-_half-08.png" style="float:right; margin-left: 10px">\nOne of the things I have really missed in Canada, is regionally based crime stories. I think Toronto has many stories waiting to be told, and not just Toronto, but specific areas - Scarborough, the Junction, white-collar crime on Bay street, corruption at Queens Park, etc. In Germany, the concept "regiokrimi" (regional crime) even has it\'s <a href="http://www.regiokrimi.de/">own website</a>. Apparently there are even small towns that commission crime novels set in their area for the town\'s anniversary. This has led to some critical voices, complaining that literary aspects are overlooked in an attempt to stuff as many local references as possible, but I\'ve come across a number of authors that I really enjoyed.</p>\n\n<p><strong><a href="http://www.klauspeterwolf.de/">Klaus-Peter Wolf</a></strong> has a fascinating history on <a href="http://de.wikipedia.org/wiki/Klaus-Peter_Wolf">Wikipedia</a>. Apparently he almost works like an investigative journalist, often diving deep into the various thematics that he then fictionalizes, going so far as to set up a fake mailorder-marriage company before he wrote a book about the trade with women, and living with a youth gang, before writing a book about youth criminality and violence. However, my favorite is his series of novels from Ost-Friesland, an area I had never heard about before, and now feel that I know well enough that I could navigate the area without a map. </p>\n\n<p>Another set of great novels set in the area come from <strong><a href="http://www.sven-koch.com/">Sven Koch</a></strong>.</p>\n\n<p><img src="/blog/images/2014-10-17-random-german-stuff-that-matters---great-content-auf-deutsch_-_whole-07.png" style="float:right; margin-left: 10px">\n<strong><a href="https://de.wikipedia.org/wiki/Jacques_Berndorf">Jaques Berndorf</a></strong> writes about another area of Germany that I was not familiar with, the Eifel. In this case, the protagonist is an investigative journalist, rather than an investigator, and somehow he always gets tangled into complex issues, often involving the state and the secret services. A cranky old man living alone in the countryside, there always seems to be an intriguing female showing up just in time to join him in his investigations. </p>\n\n<p>Reminds me of some earlier (80\'s-90\'s) Swedish crime, which also talked about investigative journalists, the Swedish secret-services and government, and their connections to foreign powers, etc. </p>\n\n<p><strong><a href="http://www.jenksaborowski.com/">Jenk Saborowski</a></strong> writes about the fictional European Federal Police, and agent Solveigh Lang, who travels Europe to catch criminals. Intriguing fiction, which reflects a modern Europe connected with high-speed rail and the absence of borders, but still divided by different bureaucracies, languages and cultures.</p>\n\n<p><img src="/blog/images/2014-10-17-random-german-stuff-that-matters---great-content-auf-deutsch_-_whole-06.png" style="float:right; margin-left: 10px"></p>\n\n<p><strong><a href="http://www.tomliehr.de/">Tom Liehr</a></strong> has been compared to Nick Hornby, and it\'s a comparison that makes a lot of sense. He writes about young men, fixated on music, DJing, being a radio host, or going on crappy chartertours. Great writing, and very rich characters.</p>\n\n<p><strong>There are so many more authors I could list, here are some very quick introductions</strong></p>\n\n<ul>\n<li>\n<strong>Nele Neuhaus</strong>: Decent crime set in "Vordertaunus", rural area near Frankfurt. Somehow always ends up with a situation in which a large number of people could have had motive and possibility to kill the victim.</li>\n<li>\n<strong>Sebastian Fitzek</strong>: "Psychodrama"-bestseller. Really enjoyed <em>Passagier 23</em> and <em>Amokspiel</em>, but others of his novels are too "psychological", with the I-voice seemingly going crazy, unable to believe anything he sees/remembers, etc. \n<img src="/blog/images/2014-10-17-random-german-stuff-that-matters---great-content-auf-deutsch_-_whole-05.png" style="float:right; margin-left: 10px">\n</li>\n<li>\n<strong>Petra Durst-Benning</strong>: Interesting and enjoyable historical fiction from various periods in Germany, usually centered around strong women. </li>\n<li>\n<strong>Birgit Schlieper</strong>: Very interesting and well written novels about difficult issues facing young people, with a strong psychological aspect. Powerful and thoughtful.</li>\n<li>\n<strong>Katharina Peters</strong>: Fascinating crime novels from the other German coast, RÃ¼gen, often involving the Nazi-history of the area.</li>\n<li>\n<strong>Martin Suter</strong>: A series of crime novels, and some free-standing novels, usually dealing with the Swiss upperclass. Charmingly written. Best free-standing novel is <em>Der Koch</em>, amazing writing (although it looses some of the steam after the beginning).\n<img src="/blog/images/2014-10-17-random-german-stuff-that-matters---great-content-auf-deutsch_-_whole-04.png" style="float:right; margin-left: 10px">\n</li>\n<li>\n<strong>Sigrid Ramge</strong>: OK crime stories set in Baden-WÃ¼rttemberg, but felt like very "regio-krimi", almost like she wants to give you a detailed tour of the area, rather than just building it into the story.</li>\n<li>\n<strong>Manuela Kuck</strong>: Well-written crime from the Wolfsburg-region. </li>\n<li>\n<strong>Sebastian Lehmann</strong> with <em>Genau mein Beutelschema</em>, hilarious and well-written ironic take on the different areas and subcultures in Berlin. Perfect reading while staying in NeukÃ¶lln last summer.</li>\n</ul>\n\n<p>Gute VergnÃ¼gen!</p>',
          title: 'Random German Stuff that Matters - great content auf Deutsch',
          date: '2015-01-23T16:42:31.000Z',
          link:
            'http://reganmian.net/blog/blog/2014/10/17/random-german-stuff-that-matters---great-content-auf-deutsch/',
          categories: [],
          blogtitle: 'Random Stuff that Matters',
          author: 'Stian HÃ¥klev'
        }
      }
    }
  },
  cjid8bdy9000uc6se91pztehm: {
    id: 'cjid8bdy9000uc6se91pztehm',
    li: {
      id: 'cjid8bdy9000fc6seaanydoki',
      liDocument: {
        liType: 'li-rss',
        createdAt: '2018-06-13T14:47:08.385Z',
        createdBy: 'op-rss',
        payload: {
          content:
            '<div style="float:right;font-size:80%;margin:30px;"><ul style="margin-top: 0em;margin-bottom: 1.5em;"><li><a href=#0>Use RStudio</a></li><li><a href=#1>Use Knitr</a></li><li><a href=#2>Learn from others</a></li><li><a href=#3>Separate cleaning and organizing from analysis</a></li><li><a href=#4>Use version control</a></li><li><a href=#5>Learn the DSLs in R</a></li><li><a href=#6>DRY - Don\'t repeat yourself</a></li><li><a href=#7>Keeping track of types in R</a></li><li><a href=#8>Asking questions - providing reproducible examples</a></li><li><a href=#9>Packages to use</a></li></ul></ul></div><p><a href="http://www.r-project.org/">R</a> is a very powerful open source environment for data analysis, statistics and graphing, with thousands of <a href="http://cran.r-project.org/web/packages/">packages</a> available. After my previous blog post about <a href="http://reganmian.net/blog/2013/10/02/likert-graphs-in-r-embedding-metadata-for-easier-plotting">likert-scales and metadata in R</a>, a few of my colleagues mentioned that they were learning R through <a href="https://class.coursera.org/compdata-003/class">a Coursera course on data analysis</a>. I have been working quite intensively with R for the last half year, and thought I\'d try to document and share a few tricks, and things I wish I\'d have known when I started out.</p>\n\n<p><img src="/blog/images/2013-10-09-starting-data-analysiswrangling-with-r-things-i-wish-id-been-told_-_whole-05.png" style="none"></p>\n\n<p>I don\'t pretend to be a statistics whiz â I don\'t have a strong background in math, and much of my training in statistics was of the social science <em>"click here, then here in SPSS"</em> kind, using flowcharts to divine which tests to run, given the kinds of variables you wanted to compare. I\'m eager to learn more, but the fact is that running complex statistical functions in R is typically quite easy. The difficult part is acquiring data, cleaning it up, combining different data sources, and preparing it for analysis (they say 90% of a data scientist\'s job is data wrangling). <em>Of course, knowing which tests to run, and how to analyze the results is also a challenge, but that is general statistical knowledge that applies to all statistics packages.</em></p>\n\n<p>So here are some of my suggestions and "lessons learnt", in no particular order. Some will find the code samples scary, others will find the suggestion to use for-loops far too basic, but hopefully you will find something useful here.</p>\n\n<!-- more -->\n\n<h1 id=0>Use RStudio</h1>\n\n<p><img src="/blog/images/2013-10-09-starting-data-analysiswrangling-with-r-things-i-wish-id-been-told-_-whole-01.png" style="none"></p>\n\n<p><a href="http://www.rstudio.com/">RStudio</a> is an great open source <em>integrated development environment</em> for R. It is free and open-source, and integrates</p>\n\n<ul>\n<li>a project manager</li>\n<li>a text editor with syntax highlighting and tab-completion</li>\n<li>package management</li>\n<li>previewing plots</li>\n<li>previewing tables (see columns and rows in your dataset)</li>\n<li>integrated help (click F1 on any function)</li>\n<li>jump to source (click F2 on any function)</li>\n<li>and <a href="http://yihui.name/knitr/">Knitr</a> (write reports in <a href="http://daringfireball.net/projects/markdown/">Markdown</a>, see below)</li>\n</ul>\n\n<p>There\'s even a version of RStudio <a href="http://www.rstudio.com/ide/docs/server/getting_started">that runs in the browser</a> â we\'re currently using it to coordinate data analysis among a geographically dispersed team on many gigabytes of data. Keeping it on a central server, and letting people run analyses directly on the data is much more convenient and secure than having everyone store tens of gigabytes of data on their personal computers.</p>\n\n<h1 id=1>Use Knitr</h1>\n\n<p><a href="http://en.wikipedia.org/wiki/Literate_programming">Literate programming</a> is the idea of mixing executable code with documentation in the same document. Knitr brings this functionality to R, and it\'s integrated beautifully with RStudio. By default, all the text you write in a Knitr document is interpreted as <a href="http://daringfireball.net/projects/markdown/">Markdown</a> (a light-weight markup language, which I\'m also using to author this blog). Press <code>Alt+Cmd I</code> to insert an R code block. You can run the code either by pressing <code>Cmd+Enter</code> on a single line, or <code>Alt+Cmd C</code> to execute an entire code block. When you\'re done, you can press "Knit HTML", which executes the whole document and produces a report.</p>\n\n<p>Here\'s an example from some recent work analyzing a questionnaire, we\'re introducing the graph, and then adding the code that will produce the graph (<a href="http://reganmian.net/blog/2013/10/02/likert-graphs-in-r-embedding-metadata-for-easier-plotting">see my previous blog post on likert-graphs</a>):</p>\n\n<p><img src="/blog/images/2013-10-09-starting-data-analysiswrangling-with-r-things-i-wish-id-been-told_-_whole-04.png" style="none"></p>\n\n<p>Running <code>Knit HTML</code> combines the text, formatted nicely, the code used to generate the graph, and the actual graph:</p>\n\n<p><img src="/blog/images/2013-10-09-starting-data-analysiswrangling-with-r-things-i-wish-id-been-told_-_whole-03.png" style="none"></p>\n\n<p>For the final report, you might choose to hide all the code segments with this invocation:</p>\n\n<pre><code class="r">library(knitr)\nopts_chunk$set(echo=F, warning=F,message=F,results="asis", prompt=F, error=F)\n</code></pre>\n\n<p>Not only is this a great way of writing reports (you can also export to PDF, or even write in LaTeX instead of Markdown), but it\'s a very nice way of organizing your code. I now do all my development in this mode, even for scripts where I\'m not interested in the final report. I like the ease of documenting, the clear visual separation of the different code blocks, and the ease of pressing <code>Alt+Cmd C</code> to execute a single code block.</p>\n\n<p><em>I recently gave a small demo to a research group, showing them RStudio, using Markdown to write knitr reports, and Shiny to make interactive webpages:</em></p>\n\n<iframe width="459" height="344" src="https://www.youtube.com/embed/LIIC8cViC54?feature=oembed" frameborder="0" allowfullscreen></iframe>\n\n<h1 id=2>Learn from others</h1>\n\n<p>There are lot\'s of R textbooks and documentation out there. Two other great sources of ideas are <a href="http://rpubs.com/">RPubs</a> and <a href="http://www.r-bloggers.com/">r-bloggers</a>. Using knitr to write reports in RStudio, as described above, you have the option to upload your report to RPubs. You can also see all the reports that others have written. These are great to learn from, since they typically contain both all the code, and the output and graphs, tying it all together into an analytical narrative. Unfortunately, there is no good way of searching the site, so you will find people\'s homework nestled between great expository writing, but it\'s still well worth a visit. You can also visit <a href="http://yihui.name/knitr/demo/showcase/">the knitr notebook showcase</a> to see some select examples.</p>\n\n<p><a href="http://www.r-bloggers.com/">R-bloggers</a> is an aggregator for blog-posts about R and statistics, and it\'s a great way to discover new packages or R features, and see how other people attack various data analysis challenges, often using publicly available datasets.</p>\n\n<p><a href="http://rpubs.com/"><img src="/blog/images/2014-10-14-starting-data-analysiswrangling-with-r-things-i-wish-id-been-told_-_whole-01.png" style="none"></a></p>\n\n<h1 id=3>Separate cleaning and organizing from analysis</h1>\n\n<p>Although R is a fully-fledged (although a bit crufty) programming language with object-orientation and functions, the code that users typically write is very different from an R package. Users usually write very imperative code, <em>"load this file, then transform the second column, then add the third column, then graph it"</em>. However, acquiring some good habits of organizing the code and working with the data, might save you a lot of time in the long run. <em>(Also see the section on DRY below)</em></p>\n\n<p>I usually separate data importing and cleaning from the analysis. My goal is to leave the raw data completely unchanged, and do all the transformation in code, which can be rerun at any time. While I\'m writing the scripts, I\'m often jumping around, selectively executing individual lines or code blocks, running commands to inspect the data in the REPL (read-evaluate-print-loop, where each command is executed as soon as you type enter, in the picture above it\'s the pane to the right), etc. But I try to make sure that when I finish up, the script is runnable by itself.</p>\n\n<p>Knitr helps impose this - when you choose <code>Knit HTML</code>, it begins with a clean slate. When you are working in RStudio, you might have objects lying around from calculations you did a while ago (with code that you\'ve already changed), but if the "knitting" is successful, you know that the current code is valid and produces exactly what you see.</p>\n\n<p><img src="/blog/images/2013-10-09-starting-data-analysiswrangling-with-r-things-i-wish-id-been-told_-_whole-06.png" style="none"></p>\n\n<p><strong>Example of preparing data</strong></p>\n\n<p>In one example, I had gotten spreadsheets from several students who had helped me enter data from a large survey. I could have opened these in Excel, and copied and pasted the information into one sheet, but instead I left the files as they were, and read them into R. I tag the columns from each spreadsheet with provenance, if I want to run any quick tests, or even more formal <a href="http://en.wikipedia.org/wiki/Inter-rater_reliability">interrater reliability tests</a>, and merge them into one table. In some spreadsheets, there was an extra empty column, so I remove that programmatically (rather than editing the Excel spreadsheet). First I load <code>xlsx</code> to read the Excel spreadsheets, and <code>plyr</code> to rename fields later.</p>\n\n<pre><code class="r">library(xlsx)\nlibrary(plyr)\n</code></pre>\n\n<p>Then I read in and join the spreadsheet files:</p>\n\n<pre><code class="r">ana &lt;- read.xlsx(file="ana.xlsx", 1,stringsAsFactors=FALSE)\nana$by &lt;- "Ana"\n\nchad &lt;- read.xlsx(file="chad.xlsx", 1,stringsAsFactors=FALSE)\nchad[["NA."]] &lt;- NULL\nchad$by &lt;- "Chad"\n\ndd &lt;- read.xlsx(file="DD.xls", 1,stringsAsFactors=FALSE)\ndd$by &lt;- "DD"\n\ndb &lt;- rbind(ana, chad, dd)\n</code></pre>\n\n<p>I noticed that some of the spreadsheets had a bunch of empty rows at the bottom. This might not be the most elegant way, but I used this function to remove all the spreadsheets with all empty values. The way this works is that is.na(x) produces a list of "TRUE TRUE FALSE FALSE TRUE" depending on which of the columns has an NA for that particular row, and sum() adds up the TRUE\'s as 1, and FALSE\'s as 0.</p>\n\n<pre><code class="r">db2 &lt;- db[apply(db,1, function(x) {\n  sum(is.na(x)) &lt; 43}),]\n</code></pre>\n\n<p>Then I move on to turn all the various versions of "empty cell" into NA:</p>\n\n<pre><code class="r">db &lt;- as.data.frame(lapply(db2, function(x){\n  x &lt;- replace(x, x %in% c("n", "N", ""), NA)\n  x &lt;- as.factor(x)}))\n</code></pre>\n\n<p>Some of the columns are demographic values, so I\'ll change these from numeric to categorical variables with the actual values:</p>\n\n<pre><code class="r">db$gender &lt;- revalue(db$X1, c("1"="Male", "2"="Female", "3"="Trans", "4"="Other", "5"=NA))\ndb$major &lt;- revalue(db$X2, c("1"="History", "2"="Religion", "3"="Other"))\n</code></pre>\n\n<p>And the rest of the questions are likert-style questions with the same categories, so I\'ll both rename them and order them in one swoop:</p>\n\n<pre><code class="r">likertcat &lt;- c("1"="Not at all", "2"="To a small extent", "3"="To some extent",\n  "4"="To a moderate extent", "5"="To a large extent")\n\nfor(e in names(db[,9:44])) {\n  db[[e]] &lt;- revalue(db[[e]], likertcat)\n  db[[e]] &lt;- ordered(db[[e]], levels= c("Not at all","To a small extent",\n    "To some extent","To a moderate extent","To a large extent"))\n}\n</code></pre>\n\n<p>I also add categories and groupings using my own addition, which <a href="http://reganmian.net/blog/2013/10/02/likert-graphs-in-r-embedding-metadata-for-easier-plotting">you can read more about</a>, and finally I save the new table as an RData file:</p>\n\n<pre><code class="r">save(db, file="db.RData")\n</code></pre>\n\n<p>I should always be able to re-run this script, and re-generate the exactly same RData file, but typically the only time I\'ll re-run the script is if I get additional data (let\'s say two other students send me their data entry files).</p>\n\n<p>I then begin the data analysis script with</p>\n\n<pre><code class="r">load(file="db.RData")\n</code></pre>\n\n<p>And we have a nicely prepared table that we can begin exploring.</p>\n\n<p><img src="/blog/images/2013-10-09-starting-data-analysiswrangling-with-r-things-i-wish-id-been-told_-_whole-07.png" style="none"></p>\n\n<h1 id=4>Use version control</h1>\n\n<p><img src="/blog/images/2013-10-09-starting-data-analysiswrangling-with-r-things-i-wish-id-been-told_-_half-02.png" style="float:left; margin-right: 10px">\nRStudio comes with support for <a href="http://git-scm.com/">git</a> baked in, and it\'s a great practice to use it. When you create a new project, check the box for <em>"Create a git repository for this project"</em>, and you get all this functionality for free. Once you have a functioning version of your script, commit it to Git, and when you later make changes, you can easily track those changes back in history, restore earlier versions, etc. This is great for people collaborating, but it\'s actually a great idea to start doing it even on solo-projects, and can save you a lot of grief in the future. RStudio has a good <a href="http://www.rstudio.com/ide/docs/version_control/overview">writeup</a> of this functionality.</p>\n\n<p><img src="/blog/images/2013-10-09-starting-data-analysiswrangling-with-r-things-i-wish-id-been-told_-_whole-08.png" style="none"></p>\n\n<p>Of course, if you are able to do so, it would be great if you could share your scripts (and data) on <a href="https://github.com/">GitHub</a> or another public repository. Even in cases where you can\'t share your raw data (because of ethics, etc), your code could be useful to others. (I have shared some of the code we used <a href="http://www.ocw.utoronto.ca/demographic-reports/">to analyze Coursera MOOC data</a> in <a href="https://github.com/houshuang/coursera-scripts">a GitHub repository</a>, but I could probably go through my folders and find more snippets of code to share).</p>\n\n<h1 id=5>Learn the DSLs in R</h1>\n\n<p>DSL stands for domain-specific language, and the key to being efficient in R (and one of the reasons beginners might feel that the learning curve is very steep), is to realize that R is actually composed of several different embedded languages. Most end-users of R actually don\'t need to know very much about the R programming language, mostly people write scripts in a very imperative way "first do this, then do this, then do that". There are probably data analysts that have used R for years, and never written a function or a loop statement.</p>\n\n<p>However, if you want to plot graphics with <a href="http://ggplot2.org/">ggplot</a>, you will have to learn an entirely new way of thinking. It is based on the <a href="http://vita.had.co.nz/papers/layered-grammar.html">Grammar of Graphics</a>, and makes it possible to intuitively build up exactly the kind of graph you want, based on layers, mapping data to geometries such as bar-charts, etc. However, if you don\'t understand the underlying logic, even making the simplest plot will seem like a strange incantation.</p>\n\n<p>Similarly, for data wrangling, there are several options. The <a href="http://www.stat.ubc.ca/%7Ejenny/STAT545A/block04_dataAggregation.html">plyr</a> family of functions is very powerful, for example here is how you would take a data.frame with per-country data, and calculate per-continent summary statistics:</p>\n\n<pre><code class="r">ddply(gDat, ~ continent, summarize,\n      minLifeExp = min(lifeExp), maxLifeExp = max(lifeExp),\n      medGdpPercap = median(gdpPercap))\n</code></pre>\n\n<p>An alternative is to use <a href="http://blog.yhathq.com/posts/fast-summary-statistics-with-data-dot-table.html">data.table</a>. Data.table is an alternative to the built-in data.frame, which is much more performant for large datasets, and also offers powerful indexing, transformation/grouping, and merging/joining. It has a very dense syntax, which is powerful once you understand the logic. Here is an example of calculating a number of statistics on a football dataset:</p>\n\n<pre><code class="r">dt.statsPlayer &lt;- dt.boxscore.subset[,list(minutes=sum(minutes),\n                                           mean=mean(pp48),\n                                           min=min(pp48),\n                                           lower=quantile(pp48, .25, na.rm=TRUE),\n                                           middle=quantile(pp48, .50, na.rm=TRUE),\n                                           upper=quantile(pp48, .75, na.rm=TRUE),\n                                           max=max(pp48)),\n                                     by=\'player\']\n</code></pre>\n\n<p>(Note also the use of white-space, R often let\'s you space expression out over several lines, which can help a lot with readability).</p>\n\n<p>The key to getting proficient in R is to choose one main way of doing things, for example ggplot2 for all graphics (and not worrying about learning grid and lattice plotting), and either data.table or plyr, and focusing on understanding their underlying logic, and using them frequently.</p>\n\n<h1 id=6>DRY - Don\'t repeat yourself</h1>\n\n<p>This used to be a mantra when programming Ruby, but is often overlooked in R code. Since many people think of doing analysis in R as simply writing a number of instructions to be executed linearly, we forget that the language let\'s us be much more efficient. I did say above that you don\'t need to understand the R programming language in depth to use R profitably, however, a few features are very useful.</p>\n\n<p>The advantage of avoiding repetition is that your code becomes easier to read (because your intention stands out), and it becomes much easier to reuse code, and to update. Let\'s say you are plotting five or six similar plots, and in each case you have a big blob of ggplot2 code, with layers, themes, etc. Now your boss tells you to change the theme of all the plots. Rather than jumping around and editing a bunch of lines, perhaps introducing new errors, you could try to abstract out the plotting code to a function, which you\'d only need to edit once.</p>\n\n<p><strong>Turning often used code into functions</strong></p>\n\n<p><a href="/blog/images/2013-10-09-starting-data-analysiswrangling-with-r-things-i-wish-id-been-told_-_half-03-orig.png"><img src="/blog/images/2013-10-09-starting-data-analysiswrangling-with-r-things-i-wish-id-been-told_-_half-03.png" style="float:right; margin-left: 10px"></a>\nHere\'s an example of a simple plot I used frequently in an exploratory report I wrote about MOOC data. I first clean out the NAs from the dataframe, and then display it as a colored bar chart, where each bar corresponds to a region, and the colors correspond to age groups. The first line defines the plot, and the two subsequent lines formats it (flipping the axis, removing some clutter, adding a legend title). <em>(Click the image to the right to see the full graph.)</em></p>\n\n<pre><code class="r">dbgen &lt;- db[!is.na(db$gender), ]\nggplot(data=dbgen, aes(region, fill=age)) + geom_bar(position="fill") +\n  coord_flip() + scale_y_continuous(labels = percent) + ylab("") + xlab("") +\n  guides(fill=guide_legend(title="Age"))\n</code></pre>\n\n<p>Since I needed a few graphs, the initial impulse would be to copy it and modify a few parameters. But how can we instead make this into a generic function? Turns out that in general we\'d want to remove NAs from both the grouping and the fill column (the region column just didn\'t happen to have any NAs, so we didn\'t need to check that above), and we also need to switch to using aes_string because of how ggplot implements its "DSL", otherwise it\'s quite simple.</p>\n\n<p>We define a function <code>my_sideplot</code>, which takes the full dataframe, the grouping variable, the fill variable, and an optional title for the fill legend, and generates a plot similar to the one above.</p>\n\n<pre><code class="r">my_sideplot = function(db, group, fill, fill_title="") {\n  db = db[!is.na(db[[fill]]) &amp; !is.na(db[[group]]),]\n  ggplot(data=db, aes_string(group, fill=fill)) +\n    geom_bar(position="fill") + coord_flip() +\n    scale_y_continuous(labels = percent) + ylab("") + xlab("") +\n    guides(fill=guide_legend(title=fill_title))\n}\n</code></pre>\n\n<p>The advantage is that I can now very easily experiment with different versions. Perhaps using the age variable for grouping, and the regions for fill is better?</p>\n\n<pre><code class="r">my_sideplot(db, "age", "region", "Regions")\n</code></pre>\n\n<p>This makes my code much easier to read, since I see right away that this is doing exactly the same as the code above (otherwise you need to look very closely at the ggplot code to see what is different), and if I have a knitr report with 10-15 of these graphs, and I want them all to use <code>theme_bw()</code> â a black-and-white theme â I can simply modify the function, and rerun the graph.</p>\n\n<p>Functions can of course be much more advanced than this, with branches and sanity checks, etc. However, as a first approximation, just taking chunks of code that are repeated with very few changes, identifying the changes and making them into parameters, and turning the code chunk into a function, can already be very useful.</p>\n\n<p><strong>Using lists and iteration</strong></p>\n\n<p>The second approach to cleaning up code is to use lists and for-loops. For example, we might want to generate plots of six different categorical variables grouped by region. Or perhaps we want them grouped by region, age, and English-skill? Now we are talking about a total of 20 graphs. We could copy and paste the one-line <code>my_sideplot</code> invocation above, which is already a big improvement, but we can do much better.</p>\n\n<pre><code class="r">group_vars = c("region", "age", "english.skill")\nfill_vars = c("usercat", "education.level", "degree.program")\nfor (groupv in group_vars) {\n  for (fillv in fill_vars) {\n    my_sideplot(db, groupv, fillv)\n  }\n}\n</code></pre>\n\n<p>This code will iterate through the <code>group_vars</code>, and for each <code>group_var</code>, it will iterate through the <code>fill_vars</code>. For each <code>fill_var</code>, it will render an appropriate plot.</p>\n\n<p>You can also iterate through for example the columns in a data.frame. Let\'s say we want to turn all the character columns into factors:</p>\n\n<pre><code class="r">for (e in names(db)) {\n  if (is.character(db[[e]])) { db[[e]] &lt;- as.factor(db[[e]])}\n}\n</code></pre>\n\n<h1 id=7>Keeping track of types in R</h1>\n\n<p>Again breaking against my statement that you don\'t need to know much of the R language, but <a href="http://www.statmethods.net/input/datatypes.html">the various data types</a> is one of the things that has tripped me up the most. This is because certain operations can return different data types than you were expecting.</p>\n\n<p>There are many R tutorials, and I am not going to give a full introduction here. The most basic datatype is the vector, and one of the peculiarities of R is that even single numbers or strings, are vectors of size one.</p>\n\n<pre><code class="r">identical(c("hi"), "hi") # =&gt; TRUE\nidentical(c(1), 1) # =&gt; TRUE\n</code></pre>\n\n<p>A dataframe is a collection of vectors. Sometimes when you do an operation on a data.frame, you might get a vector back, or a matrix. You can usually cast one data type to another with <code>as.*</code>, for example <code>as.data.frame()</code></p>\n\n<pre><code class="r">&gt; as.data.frame(list("A" = c(1,2,3), "B" = c(2,3,4)))\n  A B\n1 1 2\n2 2 3\n3 3 4\n</code></pre>\n\n<p>You can check the class of an object with <code>class()</code>,</p>\n\n<pre><code class="r">&gt; class(db)\n[1] "data.frame"\n&gt; class(db$age)\n[1] "ordered" "factor"\n</code></pre>\n\n<p>If you run into weird bugs, make sure that every step of the pipeline returns the data type that the next function is expecting (or explicitly cast). Also remember that certain functions work differently depending on different data types. For example, <code>length()</code> on a list or vector returns the number of elements. <code>length()</code> on a data.frame returns the number of columns, if you want the number of rows, you need <code>nrow()</code>.</p>\n\n<h1 id=8>Asking questions - providing reproducible examples</h1>\n\n<p>There are many great resources for getting help with R, including all the books I showed pictures of at the beginning of this post. An advantage of R being text-oriented is that it is easy to paste exactly the code that is misbehaving, and for others to help out. <a href="http://stackoverflow.com/questions/tagged/r">StackOverflow</a> has a very active R community, however you have a much higher chance of getting help if you make your problem reproducible.</p>\n\n<p>Sometimes you can use the built-in data sets in R, which you can list with the command <code>data()</code>. For example, with the command <code>data(swiss)</code>, I load a dataset with data on fertility and education in Swiss cantons. I can use this to illustrate a problem, and anyone else using R will have the same dataset, and can perfectly reproduce my problem, or check that their solution works correctly.</p>\n\n<p>If none of the built-in datasets work, you can create a minimal example dataset that illustrates your problem. Let\'s say we had a problem with the <code>a</code> data.frame we constructed above. The command <code>dput(a)</code> gives us</p>\n\n<pre><code class="r">structure(list(A = c(1, 2, 3), B = c(2, 3, 4)), .Names = c("A",\n"B"), row.names = c(NA, -3L), class = "data.frame")\n</code></pre>\n\n<p>We can now construct our example:</p>\n\n<pre><code class="r">a = structure(list(A = c(1, 2, 3), B = c(2, 3, 4)), .Names = c("A",\n"B"), row.names = c(NA, -3L), class = "data.frame")\n\nggplot(a, aes(x=A, y=B)) + geom_point()\n</code></pre>\n\n<p>...and anyone can paste this into their R interpreter, and see the same output that you get. <a href="http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example">Longer explanation</a> and here\'s <a href="http://stackoverflow.com/questions/1299871/how-to-join-data-frames-in-r-inner-outer-left-right">a small example</a>.</p>\n\n<h1 id=9>Packages to use</h1>\n\n<p>One of the strengths of R is the incredible selection of packages available, but this can also be quite disorienting to a beginner. Here are just a few packages that are often useful:</p>\n\n<table>\n<thead>\n<tr>\n<th style="text-align: left">Package</th>\n<th style="text-align: left">Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style="text-align: left"><a href="http://plyr.had.co.nz/">plyr</a></td>\n<td style="text-align: left">Plyr applies the "split-apply-combine" approach to R data. You might want to calculate the average height of players, grouped by nationality and year of birth - a one-liner in Plyr. Hadley Wickham is currently developing <a href="https://github.com/hadley/dplyr">dplyr</a>, a faster and more powerful version</td>\n</tr>\n<tr>\n<td style="text-align: left"><a href="http://shiny.rstudio.com/">Shiny</a></td>\n<td style="text-align: left">Shiny let\'s you make interactive graphical websites with R (similar to the way we created the function above, by substituting the parts of the code that vary with variables, and letting the end-user choose the variables). Also see the <a href="iframe%20width=%22459%22%20height=%22344%22%20src=%22https://www.youtube.com/embed/LIIC8cViC54?feature=oembed%22%20frameborder=%220%22%20allowfullscreen%3E%3C/iframe">video</a> above</td>\n</tr>\n<tr>\n<td style="text-align: left"><a href="cran.r-project.org/package=data.table">data.table</a></td>\n<td style="text-align: left">As mentioned above, a much faster version of data.frame, very well suite to larger datasets, with powerful split+apply, merge, etc. functionality offering an alternative to plyr</td>\n</tr>\n<tr>\n<td style="text-align: left"><a href="http://jason.bryer.org/likert/">likert</a></td>\n<td style="text-align: left">If you ever deal with likert-style questionnaire data, this package makes it very easily to visualize the results. See also <a href="http://reganmian.net/blog/2013/10/02/likert-graphs-in-r-embedding-metadata-for-easier-plotting">my addition</a> to this package</td>\n</tr>\n</tbody>\n</table>\n\n<p>Hope that was useful. Once you want more, you could do worse than checking out Hadley Wickham\'s <a href="http://adv-r.had.co.nz/">Advanced R</a> book. There\'s also an incredible amount of good textbooks, examples, blog posts, etc.</p>',
          title:
            "Starting data analysis/wrangling with R: Things I wish I'd been told",
          date: '2014-10-14T16:17:31.000Z',
          link:
            'http://reganmian.net/blog/blog/2014/10/14/starting-data-analysiswrangling-with-r-things-i-wish-id-been-told/',
          categories: [],
          blogtitle: 'Random Stuff that Matters',
          author: 'Stian HÃ¥klev'
        }
      }
    }
  },
  cjid8bdy9000vc6sergycivbm: {
    id: 'cjid8bdy9000vc6sergycivbm',
    li: {
      id: 'cjid8bdy9000gc6serc05sdks',
      liDocument: {
        liType: 'li-rss',
        createdAt: '2018-06-13T14:47:08.385Z',
        createdBy: 'op-rss',
        payload: {
          content:
            '<p><strong>I describe a script aimed at supporting student idea convergence through tagging Etherpad content, and discuss how it went when I implemented it in a class</strong></p>\n\n<h1>Background</h1>\n\n<p>In <strong><a href="/blog/2014/10/03/supporting-idea-convergence-through-pedagogical-scripts/">an earlier blog post</a></strong> I introduced the idea of pedagogical scripting, as well as implementing scripts in computer code. I discussed my desire to make ideas more "moveable", and support deeper work on ideas, and talked about the idea of using tags to support this. Finally, I introduced the tool <a href="http://reganmian.net/blog/2012/06/13/tag-extract-a-tool-to-automatically-restructure-textoutline-using-tags/">tag-extract</a>, which I developed to work on <a href="http://reganmian.net/wiki/draft_literature_review_open_courses">a literature review</a>.</p>\n\n<h1>Context</h1>\n\n<p><img src="/blog/images/2014-10-03-a-pedagogical-script-for-idea-convergence-through-tagging-etherpad-content_-_half-02.png" style="float:right; margin-left: 10px">I am currently teaching a course on Knowledge and Communication for Development (<a href="http://idsb10.pbworks.com/w/page/37652952/FrontPage">earlier open source syllabi</a>) at the University of Toronto at Scarborough. The course applies theoretical constructs from development studies to understanding the role of technology, the internet, and knowledge in international development processes, and as implemented in specific development projects.</p>\n\n<p><img src="/blog/images/2014-10-03-a-pedagogical-script-for-idea-convergence-through-tagging-etherpad-content_-_half-01.png" style="float:left; margin-right: 10px">I began the course with two fairly tech-centric classes, because I believe that having an intuition about how the Internet works, is important for subsequent discussions. I\'ve also realized in previous years that even the "digital generation" often has very little understanding of what happens when you for example send a Facebook message from one computer to another.</p>\n\n<!-- more -->\n\n<p>So I spent the first week focused on the physical infrastructure of the Internet, the history and development, protocols and structure. We used traceroute to explore connections between local computers, from Scarborough to downtown Toronto, or to servers in the US or China. We used telnet to simulate a connection to a web server, we handwrote a very simple webpage, and I had students in groups "draw" the process of sending an e-mail from Toronto to China (inspired by <a href="http://openmatt.org/2010/10/04/draw-how-the-internet-works/">John Britton\'s idea</a> in his <a href="http://archive.p2pu.org/webcraft/web-200-anatomy-request.html">P2PU course</a>)</p>\n\n<p><img src="/blog/images/2014-10-03-a-pedagogical-script-for-idea-convergence-through-tagging-etherpad-content_-_half-03.png" style="float:right; margin-left: 10px">The second week, <strong>in which I implemented this script</strong>, was focused more on the software side, particularly Web 2.0. I wanted the students to understand the history of the Web, the concept of the social web, how websites have gone from simple containers of content to web apps and places of interaction, the interaction between commercial providers and user-generated content, etc. <em>I also thought this would be a great opportunity to try out some interactive production and organization of knowledge in practice.</em></p>\n\n<h1>The script</h1>\n\n<p>My basic idea was to have the students work in small groups on adding ideas to group Etherpads. My key question was: <strong>What are the distinguishing features of Web 2.0</strong>. I would have a few different "external stimuli", after which they would add more ideas to their own pad. At the end of the class, we would use tagging to create a <a href="http://en.wikipedia.org/wiki/Folksonomy">folksonomy</a>, and then collaboratively reduce the folksonomy to a few key categories. Based on the categories, my script would extract the tagged pieces of text from everyone\'s pad, and create one new pad per category. My idea was then to assign the students to edit these pads and turn them into wiki articles that students could later refer to when studying, or working on the final project.</p>\n\n<p>Inspired by Pierre Dillenbourg\'s work on orchestration graphs, which he talked about at <a href="https://www.youtube.com/watch?v=UIRDxHAnAlg">ICLS in Sydney</a> and <a href="http://new.livestream.com/hgselive/events/3105335/videos/55325177">LASI</a> in Cambridge, MA, I attempted to visually represent the script.</p>\n\n<p>Script (click for a bigger version)</p>\n\n<p><a href="/blog/images/2014-10-03-a-pedagogical-script-for-idea-convergence-through-tagging-etherpad-content_-_whole-04-orig.png"><img src="/blog/images/2014-10-03-a-pedagogical-script-for-idea-convergence-through-tagging-etherpad-content_-_whole-04.png" style="none"></a></p>\n\n<p>The graph is organized across three vertical levels, representing work done by students individually, in small teams (in our case groups of 2-3 students), and done in the whole class. The x-axis represents time, and the stars represents processing by an external script which shuffles information between pads. Below, I\'ll discuss the script more in detail.</p>\n\n<h2>Phase 1</h2>\n\n<p><img src="/blog/images/2014-10-03-a-pedagogical-script-for-idea-convergence-through-tagging-etherpad-content_-_whole-03.png" style="float:left; margin-right: 10px">The students had been assigned two readings as homework, and came to the class prepared. In addition, they mostly had pre-existing ideas about Web 2.0, the social web, etc. Before doing any lecturing at all, I asked them to gather into groups of 2-3 people (based on where they were sitting), and counted off. A script had already generated a number of Etherpads with identical prompts, that were all linked from the week\'s wiki page.</p>\n\n<p><img src="/blog/images/2014-10-03-a-pedagogical-script-for-idea-convergence-through-tagging-etherpad-content_-_whole-05.png" style="none"></p>\n\n<h2>Phase 2</h2>\n\n<p><img src="/blog/images/2014-10-03-a-pedagogical-script-for-idea-convergence-through-tagging-etherpad-content_-_whole-02.png" style="none"></p>\n\n<p>The idea was to have a few rounds of providing the students with stimulus that would lead to the generation of more ideas, which could be recorded in the pads. After the students had finished their initial brainstorming, I led the class in a discussion, talked through the articles, and showed two Michael Wesch videos: <a href="http://www.youtube.com/watch?v=NLlGopyXT_g">The Machine is Us/ing Us</a> and <a href="https://www.youtube.com/watch?v=-4CV05HyAbM">Information R/evolution</a>. I then gave the students some time to write down ideas and thoughts.</p>\n\n<p><img src="/blog/images/2014-10-03-a-pedagogical-script-for-idea-convergence-through-tagging-etherpad-content_-_half-04.png" style="float:right; margin-left: 10px">I wanted to give the students a sense of how the web has developed, and I\'ve found the <a href="https://archive.org/web/">Internet Archive Wayback Machine</a> a great tool in the past. This let\'s the students explore the history of websites they are familiar with. We looked at a few examples together, and then I asked each group to choose a few websites they were familiar with (and encouraged them to also look at websites in other languages, since we have a very multilingual student body), and specifically look at features and functionality that has changed, not just design. We then discussed some of these examples together, and the students added more ideas to their pads.</p>\n\n<p>The final stimulus was looking in-depth at a specific Web 2.0 site. I had prepared a list of URLs, and the script automatically inserted a random URL at the bottom of each group\'s pad. The URLs included Wikipedia, Flickr, a newspaper, the university LMS, etc. I always enjoy walking around the room when students are working on these kinds of tasks, because the quick conversations we have while they are in the middle of exploring a topic, and I am able to point them in the right direction, seem really valuable. (And something I have not really captured in the orchestration graph).</p>\n\n<h2>Phase 3</h2>\n\n<p><img src="/blog/images/2014-10-03-a-pedagogical-script-for-idea-convergence-through-tagging-etherpad-content_-_whole-01.png" style="none"></p>\n\n<h2>Individual tagging</h2>\n\n<p>At this point, each student group had a pad filled with notes and ideas about Web 2.0 features and functionality, the shift from Web 1.0 to Web 2.0, etc. They had gradually added to this pad during the class, and I had provided rich external input, and opportunities to discuss in small groups, and in the whole class. If the class had ended here, I would have already considered it a success.</p>\n\n<p>However, one of my design goals for this iteration was to make content generated in class more accessible and useful to students going forwards. Nobody would probably wade through a bunch of unorganized Etherpads when they were later working on the final project, or preparing for the exam. I wanted to see if my <a href="http://reganmian.net/blog/2012/06/13/tag-extract-a-tool-to-automatically-restructure-textoutline-using-tags/">tag-extract</a> approach, which had been so useful to me in organizing and structuring my own notes and ideas when doing a literature review, could help the students collaboratively reorganize all of their notes into a coherent whole.</p>\n\n<p><img src="/blog/images/2014-10-03-a-pedagogical-script-for-idea-convergence-through-tagging-etherpad-content_-_half-05.png" style="float:right; margin-left: 10px">I first asked the students to add tags to the ideas in their pads. They were all familiar with tagging from Twitter, and from the readings and videos in class, and I connected the exercise to this, without telling them about the next step yet.</p>\n\n<p><strong>Below you can see an example of the evolution of the pad from one student group, ending with the students adding tags to their existing ideas.</strong></p>\n\n<p><img src="/blog/images/2014-09-22-tagging-and-convergence-for-small-groups-collaboration-with-multiple-etherpad-01-static.png" alt="" class="post-thumb" onclick=\'this.src="/blog/images/2014-09-22-tagging-and-convergence-for-small-groups-collaboration-with-multiple-etherpad-01.gif"\'></p>\n\n<h2>Collectively organizing the folksonomy</h2>\n\n<p>After the students had added tags to their pads, the script extracted all the tags, and created a new Etherpad listing them all. Including misspellings and variants, the students had come up with almost 100 tags. The pad with all the tags offered a simple way of categorizing tags, if you have a list of tags that are similar and should be grouped together, such as this list from our class:</p>\n\n<ul>\n<li>advertisments</li>\n<li>viralmarketing</li>\n<li>consumerism</li>\n<li>marketization</li>\n<li>onlinerevenue</li>\n</ul>\n\n<p>We just needed to combine them together into one category:</p>\n\n<pre><code>advertisments: viralmarketing, consumerism, marketization, onlinerevenue\n</code></pre>\n\n<p>Because we were using Etherpad for this as well, the whole class could participate, and in just a few minutes, we had organized the tags into a much smaller number of categories. In the example above, this was quite easy, but for others, the ordering might have been a bit arbitrary. Since we did not have access to the actual text tagged at this point, it was also sometimes hard to know what people had meant.</p>\n\n<p><strong>See the animation below of this process.</strong></p>\n\n<p><img src="/blog/images/2014-09-22-tagging-and-convergence-for-small-groups-collaboration-with-multiple-etherpad-02-static.png" alt="" class="post-thumb" onclick=\'this.src="/blog/images/2014-09-22-tagging-and-convergence-for-small-groups-collaboration-with-multiple-etherpad-02.gif"\'></p>\n\n<h2>Putting it all together</h2>\n\n<p><img src="/blog/images/2014-10-03-a-pedagogical-script-for-idea-convergence-through-tagging-etherpad-content_-_half-06.png" style="float:right; margin-left: 10px">The final move was to run a script which would go through all the pads, and extract all the text tagged into appropriate category pads (ie. above, text tagged either #advertisments or #viralmarketing or #consumerism would all end up in a pad called advertisments). There was a bit of an anti-climax as that script failed to run at the end of the class, because of a silly error. That was OK, because when I fixed the script after class, and ran it, I realized that the output was often not as useful as I had naively hoped.</p>\n\n<p>On the right, you see an example of a pad, with the title listing all the tags in the category, and the specific category also appended after each text snippet. (I could also have appended the name of the group pad that the snippet came from, but in this case, that would just make it more cluttered.)</p>\n\n<h1>Evaluation</h1>\n\n<p>The first part of the script worked great, but was not that different from the way I often teach. Pushing different URLs to each pad worked very well (and students loved having something suddenly show up beneath the text they were just editing), I\'ve used this particular script again in a later class.</p>\n\n<p>However, the grand idea of using tags to reorganize and structure the text still needs more work. I think the prompt was the first problem, "What distinguishes Web 2.0" was not perhaps the most productive or clear question I could have asked. I could have made it more problem-oriented, or debate (pro/con), etc. I also never explained to the students the purpose of what we were doing - they are used to taking notes in the Etherpad during group work, but I never told them that it would be tagged, cut up into small pieces, and shared.</p>\n\n<h2>Now do this, then do that</h2>\n\n<p><img src="/blog/images/2014-10-03-a-pedagogical-script-for-idea-convergence-through-tagging-etherpad-content_-_half-07.png" style="float:right; margin-left: 10px">Once we tagged, I didn\'t tell them that we would gather all the tags and organize them, and while we were organizing the tags, I didn\'t tell them what we were doing it for. I think teachers often do this unconsciously â lead students through activities that they have clearly thought out, without telling the students where they are going, or why they are moving in a certain way. Sometimes this is a necessary part of the script, whereas at other times, students need conscious training in certain epistemic moves, to be able to carry them out effectively.</p>\n\n<p>I was very impressed by Kate Bielaczyc\'s presentation at ICLS 2010, where she used a sport metaphore to distinguish between practice and playing a match. To get good at a sport, you don\'t just play matches, the coach leads you into targetted practice, setting up contrived situations to challenge you. Similarly, she had her students practice the act of knowledge building with contrived situations, before they actually got into authentic knowledge building.</p>\n\n<blockquote>\n<p>Hypothetical game-configurations are used to reflect on the knowledge building moves made possible by a particular configuration of knowledge objects. The configurations consist of âsnapshotsâ of hypothetical student work in Knowledge Forum, meant to capture game play at a fixed point in time in order to engage the community in asking: given this configuration, what types of knowledge building moves would best contribute to advancing our knowledge? Some configurations focus on single moves, such as presenting a possible initial explanation generated in response to the problem the students are working on.</p>\n\n<p>Students then generate a knowledge building move meant to advance this initial idea. There are also more complex game configurations that present not only a possible initial explanation in response to a problem, but also provide a series of possible knowledge building moves. In this case, students both evaluate the quality of the provided moves and generate a possible next move that contributes to the progressive improvement of ideas. In all cases, students each work on the same hypothetical game-configurations so that they can then compare and contrast their proposed knowledge building moves in whole-class discussions about issues such as what makes a âgood contributionâ and what does it mean to advance the community understanding.\n(<a href="http://dl.acm.org/citation.cfm?id=1854471&amp;dl=ACM&amp;coll=DL&amp;CFID=579302205&amp;CFTOKEN=60206663">Bielaczyc &amp; Ow, 2010</a>)</p>\n</blockquote>\n\n<p>So perhaps if I did this again, with a better prompt, and students knew that they were taking notes that would be cut apart and reassembled, they would write differently. However, it is also possible that this kind of reorganization is not as useful for collaborative work â part of why it works when working on my own ideas, is that I wrote the notes myself, and I remember why I wrote them. Looking at other people\'s notes is difficult at best, and especially when they are out of context.</p>\n\n<h1>Future work</h1>\n\n<p>I still believe that Etherpad and the wiki, which is also scriptable, has a lot of potential for rapid experimentation with collaborative scripts. We have experimented with a number of other scripts in the two classes that I teach/co-teach, and I might blog about other ones later. I\'ve used a simplified version of the tagging script in two later classes, where I focused on the ability to pick out and aggregate ideas based on pre-determined tags. <em>The fact that Etherpad records the entire history, and let\'s me go back, is also useful for research, and I plan to spend a bit longer looking at how the individual team pads evolved during the different stages of the script.</em></p>\n\n<p><img src="/blog/images/2014-10-03-a-pedagogical-script-for-idea-convergence-through-tagging-etherpad-content_-_half-08.png" style="float:right; margin-left: 10px">The week after the script above, we talked about theoretical perspectives on development and technology, and as an ice-breaker before the lecture, I asked them to write down what they thought poverty was, and what they perceived the "official" definition to be. After some group discussion, I asked each group to come up with a single sentence for each, and tag them with respectively #ithink and #worldthinks. The script then simply aggregated all of these on a simple webpage, which was a great way of exploring different approaches (ethics, capability approach, materialistic, relative/objective poverty, happiness, etc).</p>\n\n<p><em>Writing these <a href="/blog/2014/10/03/supporting-idea-convergence-through-pedagogical-scripts/">two</a> blog posts has also been interesting. I know they ended up very long, and kind of mix technical stuff, CSCL theory, my own ideas about how people work with ideas, and my practical experience in the class. But this is how I think â and it\'s liberating to not be constrained by the academic paper format, especially in the exploratory phase. Writing it down enables me to reflect more deeply on my own design, and execution. Putting the script into Dillenbourg\'s orchestration graph format was also a great exercise, even though I have barely scratched the surface of his framework.</em></p>\n\n<p><em>The drawing of an internet workflow comes from <a href="img%20src=\'/blog/images/2014-10-03-a-pedagogical-script-for-idea-convergence-through-tagging-etherpad-content_-_half-02.png\'%20style=\'none\'">a student in the P2PU course</a>. The Web 2.0 image comes from <a href="http://www.nptechforgood.com/2010/01/28/web-1-0-web-2-0-and-web-3-0-simplified-for-nonprofits/">Nonprofit Tech for Good</a>. The Do as I tell You sign comes from <a href="http://icehousecrafts.com/item_371/If-You-Would-Just-Do-What-I-Tell-You-I-Wouldnt-Have-To-Be-So-Bossy-Sign.htm">Ice House Crafts</a></em></p>',
          title:
            'A pedagogical script for idea convergence through tagging Etherpad content',
          date: '2014-10-03T21:17:26.000Z',
          link:
            'http://reganmian.net/blog/blog/2014/10/03/a-pedagogical-script-for-idea-convergence-through-tagging-etherpad-content/',
          categories: [],
          blogtitle: 'Random Stuff that Matters',
          author: 'Stian HÃ¥klev'
        }
      }
    }
  },
  cjid8bdy9000wc6seupz86two: {
    id: 'cjid8bdy9000wc6seupz86two',
    li: {
      id: 'cjid8bdy9000hc6se23ugkrxl',
      liDocument: {
        liType: 'li-rss',
        createdAt: '2018-06-13T14:47:08.385Z',
        createdBy: 'op-rss',
        payload: {
          content:
            '<p><strong>We can script Etherpad to push discussion prompts out to many small groups, and then pull the information back. Using tags, we can extract information, and as a community organize the emerging folksonomy</strong></p>\n\n<p><em>This blog post brings together two long-standing interests of mine. The first is how to script web tools to support small-group collaborative learning, and the other is how to support reorganization of ideas across groups.</em></p>\n\n<h1>Two meanings of the word "scripting"</h1>\n\n<p>There is an interesting intersection between two quite different meanings of the word scripting in the context of my work. In the CSCL literature, <a href="http://edutechwiki.unige.ch/en/CSCL_script">scripts</a> refer to sequences of activities that support groups of students in carrying out a collaborative learning activity. They can be very simple and generic, like the well-known <a href="http://olc.spsd.sk.ca/DE/PD/instr/strats/jigsaw/">jigsaw script</a>, or very content-specific. There is active on-going research on scripting, including <a href="http://epub.ub.uni-muenchen.de/14328/">how external scripts interfer with internal scripts</a>, and <a href="http://hal.archives-ouvertes.fr/docs/00/19/02/30/PDF/Dillenbourg-Pierre-2002.pdf">the dangers of over-scripting</a>.</p>\n\n<p><img src="/blog/images/2014-09-22-tagging-and-convergence-for-small-groups-collaboration-with-multiple-etherpad_-_whole-02.png" style="none"></p>\n\n<!-- more -->\n\n<p>From the technology side, we talk about writing scripts as a light-weight form of programming, to ask the computer to do something for us. Scripting a program, or a web site, means that we can interact with it automatically, for example asking Google Docs to set up 20 documents for us. There is obviously a connection between developing educational scripts and actually "implementing" them in technology. Often, specialized software has been written to enable specific scripts, but there has been some attempts at enabling more generic implementations of learning designs into software.</p>\n\n<p>One inspiring example for me has been <a href="http://www.gsic.uva.es/glueps/">GLUE-PS</a> (<a href="http://libgen.org/scimag/get.php?doi=10.1016/j.compedu.2013.12.008">paper</a>), which can take a learning design from a learning design software, like <a href="http://compendiumld.open.ac.uk/">CompendiumLD</a>, and set-up the necessary software, for example pre-generating Google Docs and wiki-pages, splitting students into groups, and assigning different resources to different groups, etc.</p>\n\n<p><img src="/blog/images/2014-09-22-tagging-and-convergence-for-small-groups-collaboration-with-multiple-etherpad_-_whole-01.png" style="none"></p>\n\n<h1>Etherpad for collaborative writing, and more</h1>\n\n<p><img src="/blog/images/2014-09-22-tagging-and-convergence-for-small-groups-collaboration-with-multiple-etherpad_-_half-01.png" style="float:right; margin-left: 10px">For those who have not heard of, or used, Etherpad, I always introduce it as "like Google Docs". It enables very easy collaborative editing of documents, where everyone logged in immediately see all changes made by other users. It features very light-weight formatting, and is not designed to generate print-ready documents, but is perfect for brainstorming, note taking, etc. It also focuses more on the live-editing situation, with the text written by each user colored differently. It can be quite mesmerizing to see a document being filled in by multiple people at the same time.</p>\n\n<p><img src="/blog/images/2014-09-22-tagging-and-convergence-for-small-groups-collaboration-with-multiple-etherpad_-_half-02.png" style="float:left; margin-right: 10px">We often used Etherpad for collaborative meetings at P2PU (<a href="http://en.flossmanuals.net/etherpad/case-studies/">case study</a>), and I also had some interesting experiences using in the P2PU course <a href="https://p2pu.org/en/groups/introduction-to-the-field-of-computer-supported-co/content/full-description/">"Intro to Computer-Supported Collaborative Learning"</a>. We began by chatting in the chat feature, but gradually migrated over to use the main editing space, which enabled us to keep multiple separate discussions going simultaneously. I have <a href="http://reganmian.net/wiki/analysis_of_cscl-intro#synchronous_meetings">written</a> and [spoken](<iframe width="459" height="344" src="https://www.youtube.com/embed/kGXRdj9F_4E?feature=oembed" frameborder="0" allowfullscreen></iframe> about this experience, and it\'s something I\'d love to experiment more with.</p>\n\n<p>For a large course I am currently helping teach, we began by using Etherpads to support note-taking in small group discussions. Setting up 10-15 Google Docs, remembering to set the sharing options correctly, and copying and pasting the URLs to the wiki was a lot of work, and it was much easier to do it with Etherpad. The students all found Etherpad very easy to use, and it was nice to be able to pull up group notes on the projector afterwards while groups were presenting their ideas to the rest of the class.</p>\n\n<h1>Scripting Etherpad</h1>\n\n<p>Etherpad has a nice simple API, which let\'s you create new pads and fill them with content, and read the content of existing pads. (Since there are no permissions or logins, that\'s basically all the actions that you would need). Since Etherpad conserves the full history of all changes, you can also access older versions of a pad. Here is how little code you need to get started.</p>\n\n<p><strong>A simple helper function to execute Etherpad API calls, and properly format the return message:</strong></p>\n\n<pre><code class="python">def run_etherpad(path, **params):\n    params[\'apikey\'] = settings.ETHERPAD_API\n    data = urlencode(dict(params)).encode(\'ascii\')\n    url = ("%s/api/1.2.10/%s" % (settings.ETHERPAD_URL, path))\n    r = json.loads(urlopen(url, data).read().decode(\'utf-8\'))\n    return(r)\n</code></pre>\n\n<p>And then I can simply execute any of the API commands listed in the <a href="http://etherpad.org/doc/v1.3.0/#index_api_methods">Etherpad documentation</a>, for example:</p>\n\n<pre><code class="python">text = run_etherpad("getText", padID=pad)\n</code></pre>\n\n<p>The first thing I did with this, was to write a little script that took a template (typically a few questions for discussion), and created a number of identical pre-populated Etherpads for the discussion groups. It then generated an HTML page with all the links, which I could copy and paste into the course wiki. This already saved a lot of time. Given that we now have a list of all the Etherpads, it\'s just as easy to pull all the text from the Etherpads back into a script, and do something with it.</p>\n\n<p>For example, I can generate a quick HTML page consisting of the text from all of the Etherpads together, which makes it much faster to quickly scan through what groups have been doing, rather than opening 15 tabs in the browser. Or I can automatically pull each group\'s page and post it to the wikipage of that group for easier access in the future (Etherpad is great for live editing, but not very trustworthy as a long-term repository of information).</p>\n\n<h1>Using tags to extract and organize ideas</h1>\n\n<p><em>A little detour about making ideas moveable, supporting organizing and synthesis</em></p>\n\n<p>One of the things we found from teaching this course last year, is that the ideas and information entered tended to be "stuck" on the week and group-specific pages. After several weeks of the course, we would have lot\'s of great reflections, notes from readings, project ideas, etc., but they were not organized in a way that was easily accessible to the students. When working on their final projects, the students were more likely to use Google, rather than accessing the common knowledge base which we had tried to structure throughout the course.</p>\n\n<p><img src="/blog/images/2014-09-22-tagging-and-convergence-for-small-groups-collaboration-with-multiple-etherpad_-_half-04.png" style="float:left; margin-right: 10px">I have long been inspired by a course that I took in the first year of my MA, which used an environment called Knowledge Forum. I made a <a href="https://vimeo.com/17143638">screencast</a> to showcase the course, and how the environment enabled us to go back to our old notes, reorganize them, and see new connections and gaps. At that time, I contrasted it with threaded discussion forums, where a post is usually "captured" in the chronology/thread where it is posted.</p>\n\n<p><img src="/blog/images/2014-09-22-tagging-and-convergence-for-small-groups-collaboration-with-multiple-etherpad_-_half-03.png" style="float:right; margin-left: 10px">Inspired by this course, and by workshop methodology, I began thinking about what I called <strong>"the cycle of divergence an convergence"</strong> which I began exploring in a <a href="http://reganmian.net/wiki/grappling_with_ideas">talk</a> and a <a href="http://reganmian.net/wiki/grappling_with_ideas-the_paper">paper</a>. I also thought that <a href=\'http://reganmian.net/wiki/tagging\'>tagging</a> could be a low-tech way to enable "movable ideas". An example script could be the following: after a class had discussed a topic for a few weeks, each week with a new "input" (stimulus), they could collectively or individually decide on a few emerging overarching themes, and go back through the posts, tagging them accordingly. The discussion forum software would then create "saved searches" for these tags -- dynamic folders containing all the posts tagged with a certain tag. Students could then revisit these posts in a new context, continue the discussion, etc.</p>\n\n<p><img src="/blog/images/2014-09-22-tagging-and-convergence-for-small-groups-collaboration-with-multiple-etherpad_-_half-05.png" style="float:left; margin-right: 10px">This turns out to be very similar to the process of qualitative research, and a methodology supported by tools like NVivo (<a href="https://www.youtube.com/watch?v=0YyVySrV2cM">good video intro</a>), where one codes individual pieces of text, and then pulls up a list of all the coded pieces for a given code, to see them in a new context. Something that might be useful for quantitative research, for organizing a literature review... and for supporting community knowledge and deep thinking in a collaborative learning situation?</p>\n\n<h1>Tag-extract, short recap</h1>\n\n<p><img src="http://reganmian.net/blog/wp-content/uploads/2012/06/Screen-Shot-2012-06-13-at-10.50.59.png" style="float:left; margin-right: 10px">Programmers both like to reinvent the wheel, and to scratch their own itch, so faced with a large amount of notes that I needed to structure into a literature review, while keeping the source-information (which paper did a certain idea come from), I experimented with tagging. (Adding some markup to the text, and then processing it with a script, is far easier than developing a new graphical interface for tagging). I had three simple rules, or principles, which turned out to be quite powerful:</p>\n\n<ol>\n<li>all the text below a source indicator "belongs" to that source, and will always be tagged as coming from that source</li>\n<li>any tag applies to the line it is on</li>\n<li>all text indented below a tagged line, belongs to that line</li>\n</ol>\n\n<p><img src="http://reganmian.net/blog/wp-content/uploads/2012/06/Screen-Shot-2012-06-13-at-10.57.31.png" style="float:right; margin-left: 10px">I wrote a <a href="http://reganmian.net/blog/2012/06/13/tag-extract-a-tool-to-automatically-restructure-textoutline-using-tags/">detailed blog-post</a> about this tool, complete with <a href="https://www.youtube.com/watch?v=NEfdPDptD5U">an extensive screencast</a>, showing the tool embedded in an academic workflow process, and <a href="https://www.youtube.com/watch?v=NEfdPDptD5U">a more focused screencast</a> focusing on the tool itself.</p>\n\n<p>At the top, you see a text file with notes, and bibdesk-identifiers as the "source". After adding tags, and running the script, I get the output seen on the right, where the notes are reordered according to tag, but with the source still there. When writing about for example metalearning, you then have all the relevant ideas/quotes, together with the article citations. Here\'s <a href="http://reganmian.net/wiki/litreview_raw_sorted">an example of raw sorted notes</a> about open courses, and <a href="http://reganmian.net/wiki/draft_literature_review_open_courses">here the resulting draft</a>.</p>\n\n<h1>Bringing all the pieces together</h1>\n\n<p>So we have introduced the idea of pedagogical scripting, as well as implementing scripts in computer code. I\'ve discussed the desire to make ideas more "moveable", and support deeper work on ideas, and talked about the idea of using tags to support this. Finally, I introduced the tool tag-extract, which I developed to work on a literature review. This blog post is already long enough, so I will write a <strong><a href="/blog/2014/10/03/a-pedagogical-script-for-idea-convergence-through-tagging-etherpad-content/">separate blog post</a></strong> about the actual design, implementation and evaluation of the pedagogical script using these ideas.</p>\n\n<p><em>The script flowchart is from an unpublished manuscript by Pierre Dillenbourg. The GLUE-PS image is from the <a href="http://www.gsic.uva.es/wikis/gs2/index.php/LDGResourcesGLUEPS">GLUE-PS website</a>, the concept map about concept maps from the <a href="http://cmapskm.ihmc.us/viewer/cmap/1064009710027_1483270340_27090">Cmap site</a>, and the NVivo image from the <a href="http://ebabbie.net/resource/NVivo/primer.html">NVivo primer</a></em></p>',
          title:
            'Supporting idea convergence through pedagogical scripts and Etherpad APIs, an introduction',
          date: '2014-10-03T20:17:26.000Z',
          link:
            'http://reganmian.net/blog/blog/2014/10/03/supporting-idea-convergence-through-pedagogical-scripts/',
          categories: [],
          blogtitle: 'Random Stuff that Matters',
          author: 'Stian HÃ¥klev'
        }
      }
    }
  },
  cjid8bdy9000xc6seg1y4c8h6: {
    id: 'cjid8bdy9000xc6seg1y4c8h6',
    li: {
      id: 'cjid8bdy9000ic6sewf7gxp1p',
      liDocument: {
        liType: 'li-rss',
        createdAt: '2018-06-13T14:47:08.385Z',
        createdBy: 'op-rss',
        payload: {
          content:
            '<p>I recently needed to call a Ruby script from Python to do some data processing. I was generating some Etherpad-scripts in Python, and needed to restructure tags (using <a href="http://reganmian.net/blog/2012/06/13/tag-extract-a-tool-to-automatically-restructure-textoutline-using-tags/">tag-extract</a>) with a Ruby script. The complication was that this script does not just return a simple string or number, but a somewhat complex data structure, that I needed to process further in the Python script.</p>\n\n<p>Luckily, searching online I came across the idea to use JSON as the interchange format, which worked swimmingly. Given that all the information was in text format, and the data structure was not that complex (just some lists and dictionaries), JSON could cope well with the complexity, and was easier to debug, since it\'s a text format. If I had had other requirements, like binary data, I would have had to investigate other data formats.</p>\n\n<!-- more -->\n\n<p><a href="https://github.com/houshuang/folders2web/blob/master/tag-extract.rb">The Ruby script</a> already had several different output options, controlled through command-line switches. I added the option to output through JSON, and the code required to do the actual output was simply:</p>\n\n<pre><code class="ruby">puts JSON.generate(tags)\n</code></pre>\n\n<p>I also reconfigured the script to accept input from standard-in:</p>\n\n<pre><code class="ruby">if ARGV.size == 0\n  a = ARGF.read\nelse\n  a = try { File.read(ARGV[1]) }\n  unless a\n    puts "Could not read input file"\n    exit\n  end\nend\n</code></pre>\n\n<p>This meant that I could simply pipe the information from the Python script through the Ruby script, writing to standard-out and reading from standard-in, not having to create any temporary files at all.</p>\n\n<p>I created a helper function in Python to let me run a command, putting into standard-out, and reading from standard-in:</p>\n\n<pre><code class="python">def apply_external(cmd_ary, out):\n    proc = subprocess.Popen(\n        cmd_ary,stdout=subprocess.PIPE,\n        stdin=subprocess.PIPE)\n    proc.stdin.write(bytes(out, "UTF-8"))\n    proc.stdin.close()\n    result = proc.stdout.read()\n    result = result.decode("UTF-8")\n    return(result)\n</code></pre>\n\n<p>and then I simply call it with the text I want transformed, and receive the transformed text structure back.</p>\n\n<pre><code class="python">def tag_extract(text):\n    ret = apply_external([\'ruby\', \'/Users/Stian/src/folders2web/tag-extract.rb\'], text)\n    return json.loads(ret)\n</code></pre>\n\n<p>Although the idea was simple, there was a bit of fiddling getting the pipes set up in Python, etc., so I figured that documenting it might be helpful to others.</p>',
          title:
            'Easy interoperability between Ruby and Python scripts with JSON',
          date: '2014-09-22T21:57:15.000Z',
          link:
            'http://reganmian.net/blog/blog/2014/09/22/easy-interoperability-between-ruby-and-python-scripts-with-json/',
          categories: [],
          blogtitle: 'Random Stuff that Matters',
          author: 'Stian HÃ¥klev'
        }
      }
    }
  },
  cjid8bdy9000yc6seg5oecq7c: {
    id: 'cjid8bdy9000yc6seg5oecq7c',
    li: {
      id: 'cjid8bdy9000jc6see1vqta58',
      liDocument: {
        liType: 'li-rss',
        createdAt: '2018-06-13T14:47:08.385Z',
        createdBy: 'op-rss',
        payload: {
          content:
            '<p><em>In which I automatically generate Anki review cards for vocabulary based on subtitles from a Russian Coursera MOOC</em></p>\n\n<h1>Learning Russian, a 10-year project</h1>\n\n<p>I have been working on my Russian on and off for many years, I\'m at the level where I don\'t feel the need for textbooks, but my understanding is not quite good enough for authentic media yet. I\'ve experimented with <a href="http://reganmian.net/blog/2012/02/02/my-one-month-russian-challenge/">readings novels in parallel</a> (English and Russian side by side, or Swedish and Russian, like on the picture), and I\'ve been listening to <a href="http://www.tasteofrussian.com/">a great podcast</a> for Russian learners.</p>\n\n<p><img src="/blog/images/2014-04-16-creating-anki-cards-for-russian-coursera-mooc-with-stemming-and-frequency-lists_-_whole-01.png" style="none"></p>\n\n<h1>Language learning with MOOCs</h1>\n\n<p><img src="/blog/images/2014-04-16-creating-anki-cards-for-russian-coursera-mooc-with-stemming-and-frequency-lists_-_half-03.png" style="float:left; margin-right: 10px">\nMOOCs can be a great resource for language learning, whether intentionally or not. At the University of Toronto, <a href="http://www.ocw.utoronto.ca/demographic-reports/">we found</a> that more than 60% of learners across all MOOCs spoke English as a second language, no doubt some of them are not just viewing the foreign language of the MOOC as a barrier, but are also hoping to improve their English. To my delight, the availability of non-English language MOOCs has been growing steadily. For example, Coursera has courses in Chinese, French, Spanish, Russian, Turkish, German, Hebrew and Arabic. There are also MOOC providers focused on specific linguistic areas, for example China\'s <a href="http://xuetangx.com/">XueTangX</a>, France\'s <a href="https://www.france-universite-numerique-mooc.fr/">UniversitÃ© NumÃ©rique</a>, and Germany\'s <a href="https://iversity.org/">iversity</a>.</p>\n\n<p>The existence of global open educational resources is nothing new. Five years ago, I wanted to share my enthusiasm about being able to "peek through the windows" of universities around the world, and edited <a href="https://www.youtube.com/watch?v=eRbWXKnxB2c">a YouTube mashup</a>. I\'ve also blogged about <a href="http://reganmian.net/blog/2010/12/07/oer-for-a-multicultural-classroom-student-as-user-and-producer">OER for a multicultural classroom</a>, and written about OER from <a href="http://reganmian.net/blog/the-chinese-national-top-level-courses-project/">China</a>, <a href="http://reganmian.net/blog/2008/12/05/worlds-largest-university-opens-almost-all-its-materials/">India</a>, <a href="http://reganmian.net/blog/2009/03/19/407-indonesian-textbooks-openly-available/">Indonesia</a>, etc.</p>\n\n<p>However, earlier OER videos tended to be full lecture recordings, with poor video and audio quality, often on proprietary and hard to use platforms, with very little facilitation for online learning. Current MOOC platforms have converged around short (5-15 minutes) videos, with high video and audio quality, and decent video players. The courses are coherent, designed for online learning, and often feature supporting readings, sometimes even subtitles, etc.</p>\n\n<h1>A Russian MOOC</h1>\n\n<p><img src="/blog/images/2014-04-16-creating-anki-cards-for-russian-coursera-mooc-with-stemming-and-frequency-lists_-_whole-02.png" style="none"></p>\n\n<p>The picture above is from a lecture in the MOOC <a href="https://class.coursera.org/historyofec-001">"ÐÑÑÐ¾ÑÐ¸Ñ ÑÐºÐ¾Ð½Ð¾Ð¼Ð¸ÑÐµÑÐºÐ¾Ð¹ Ð¼ÑÑÐ»Ð¸"</a> (History of Economic Thought), which is currently in it\'s second week on Coursera. I signed up for this several months ago, because I wanted to try out my Russian, and I find the topic very interesting. (Extra interesting, of course, is the question of whether this topic will be taught any differently from a country that has a very unique history with regard to economic thought and experiments). The fact that I know something about the topic, and about European history in general, makes it easier to follow the lectures, as do the subtitles, the ability to regulate the speed, and the occasional illustrations in the video. Watching a few of the videos, I found that I could more or less follow along, but there were a number of words that seemed important, and which I could not understand.</p>\n\n<h1>Stemming and word frequencies</h1>\n\n<p>I\'ve always been interested in language technology, and last year I spent some time experimenting with different libraries to create word frequency lists from Russian electronic texts. I used a library to "stem words" -- reducing all the conjugated forms of a word to a single word -- for example all these forms of the word <code>big</code>: <em>Ð±Ð¾Ð»ÑÑÐ°Ñ, Ð±Ð¾Ð»ÑÑÐµ, Ð±Ð¾Ð»ÑÑÐµÐ¹, Ð±Ð¾Ð»ÑÑÐ¸Ðµ, Ð±Ð¾Ð»ÑÑÐ¸Ð¼, Ð±Ð¾Ð»ÑÑÐ¸Ð¼Ð¸, Ð±Ð¾Ð»ÑÑÐ¾Ðµ, Ð±Ð¾Ð»ÑÑÐ¾Ð¹, Ð±Ð¾Ð»ÑÑÑÑ</em>. There are several possible libraries for doing this, none are perfect, but most are quite good. In <a href="http://reganmian.net/blog/2013/10/17/playing-with-word-stemming-and-frequencies-in-russian/">my previous blog post</a>, I used the Ruby gem <code>ruby-stemmer</code>, however when I revisited my code, I noticed that I had switched to using <code>hunspell</code> after writing the blog post.</p>\n\n<p><code>Hunspell</code> is an open-source spellchecker, which is also able to output word stems. In the example below, I simply send a Russian sentence through <code>hunspell</code>, and get back a list with the original conjugated word on the left, and the stem on the right (I\'ve removed the words that didn\'t change)</p>\n\n<pre><code class="bash">â¯â¯â¯ echo "Ð¡ÐµÐ³Ð¾Ð´Ð½Ñ Ð½Ð°ÑÐµ Ð¿ÑÐµÐ´ÑÑÐ²Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ð¾Ð± ÑÐºÐ¾Ð½Ð¾Ð¼Ð¸ÐºÐµ Ð¾ÑÐµÐ½Ñ ÑÐ¸Ð»ÑÐ½Ð¾ Ð¾ÑÐ»Ð¸ÑÐ°ÐµÑÑÑ Ð¾Ñ ÑÐ¾Ð³Ð¾,\n    ÐºÐ°Ðº Ð»ÑÐ´Ð¸ Ð´ÑÐµÐ²Ð½Ð¾ÑÑÐ¸ ÑÑÑ, Ð¼ÑÑÐ»Ð¸Ð»Ð¸ ÑÐ¾, ÑÑÐ¾ Ð¼Ñ ÑÐµÐ³Ð¾Ð´Ð½Ñ Ð½Ð°Ð·ÑÐ²Ð°ÐµÐ¼ ÑÐºÐ¾Ð½Ð¾Ð¼Ð¸ÐºÐ¾Ð¹." |\n    hunspell -d ru_RU,ru_RU_google -s\n\nÑÐºÐ¾Ð½Ð¾Ð¼Ð¸ÐºÐµ ÑÐºÐ¾Ð½Ð¾Ð¼Ð¸ÐºÐ°\nÑÐ¸Ð»ÑÐ½Ð¾ ÑÐ¸Ð»ÑÐ½ÑÐ¹\nÐ¾ÑÐ»Ð¸ÑÐ°ÐµÑÑÑ Ð¾ÑÐ»Ð¸ÑÐ°ÑÑÑÑ\nÐ´ÑÐµÐ²Ð½Ð¾ÑÑÐ¸ Ð´ÑÐµÐ²Ð½Ð¾ÑÑÑ\nÐ¼ÑÑÐ»Ð¸Ð»Ð¸ Ð¼ÑÑÐ»Ð¸ÑÑ\nÐ½Ð°Ð·ÑÐ²Ð°ÐµÐ¼ Ð½Ð°Ð·ÑÐ²Ð°ÑÑ\nÑÐºÐ¾Ð½Ð¾Ð¼Ð¸ÐºÐ¾Ð¹ ÑÐºÐ¾Ð½Ð¾Ð¼Ð¸ÐºÐ°\n</code></pre>\n\n<h1>Getting word frequencies from a MOOC subtitle file</h1>\n\n<p><img src="/blog/images/2014-04-16-creating-anki-cards-for-russian-coursera-mooc-with-stemming-and-frequency-lists_-_whole-03.png" style="float:left; margin-right: 10px">\nNot only did I have this old project with stemming and creating frequency lists of Russian words, but I have recently done some experiments with downloading video subtitles from Coursera courses for use in a machine learning project. <a href="https://github.com/houshuang/russian_stemmer/blob/master/parse_subtitles.py">This script</a> takes an HTML file with the list of course lectures (you need to be logged in to get to this page, so the easiest is to open it in the browser, and save the page), extracts all the subtitle links, downloads them, and concatenates them to one output file.</p>\n\n<p>We can then pass this file through the stemmer (all code for this blogpost on <a href="https://github.com/houshuang/russian_stemmer/">Github</a>), and generate a list of all word-stems that occur more than five times, sorted by general frequency. (I use a Russian frequency word list I found <a href="http://bokrcorpora.narod.ru/frqlist/frqlist-en.html">here</a>). The subtitles for the first two weeks of videos (18 videos) total 27,600 words (50-80 pages of printed text). There are almost 6,400 unique words (of which ÑÐµÐ»ÑÑÐºÐ¾ÑÐ¾Ð·ÑÐ¹ÑÑÐ²ÐµÐ½Ð½Ð¾Ð³Ð¾ -- agricultural -- is the longest), but only 3374 stems. By removing all stems that occur less than five times, we\'re down to a much more manageable 839 stems.</p>\n\n<p><img src="/blog/images/2014-04-16-creating-anki-cards-for-russian-coursera-mooc-with-stemming-and-frequency-lists_-_half-01.png" style="float:right; margin-left: 10px">\nI opened these in a text editor, and manually removed words I already knew (I left in many words which I kind of knew, but would like to learn more precisely). Since the words were sorted in order of global frequency, with more "rare" words near the top, I could remove most of the lower half of the file in one swoop (words like "I", "like", "and", etc). Within half an hour, I had now gone from 27,600 words to 184 word stems that I needed to learn or study.</p>\n\n<p>No problem, just copy the whole list of Russian words into Google Translate, and out comes an equally long list of English corresponding words, which I save to another text file. (These are not 100% perfect, but in my experience, it works for 95% of the words).</p>\n\n<h1>Bringing it all together - Anki and spaced repetition</h1>\n\n<p><img src="/blog/images/2014-04-16-creating-anki-cards-for-russian-coursera-mooc-with-stemming-and-frequency-lists_-_half-02.png" style="float:right; margin-left: 10px"></p>\n\n<p>A common concept on language learning blogs is <a href="http://en.wikipedia.org/wiki/Spaced_repetition">"spaced repetition"</a>, the concept of having a computer automatically determine the optimal repetition rate to counteract the natural rate of forgetting. <a href="http://ankisrs.net/">Anki</a> is a great free program for practicing flashcards using spaced repetition, with very powerful options to create your own cards, configuring the rate of repetition, etc. I have only ever used it before with public decks, but I thought I would try to use the tools I introduced above to make my own cards.</p>\n\n<p><img src="/blog/images/2014-04-16-creating-anki-cards-for-russian-coursera-mooc-with-stemming-and-frequency-lists_-_half-04.png" style="float:left; margin-right: 10px">\nThe simplest cards can be represented as two columns in a file, the front and the back. You are presented with the word on the front, try to remember the word on the back, turn the card, and indicate whether you were successful or not. However, Anki supports much more powerful card configurations. You create a new note type, with arbitrary many fields, and then you can create different card designs, incorporating these designs.</p>\n\n<p>I decided that it would be very helpful to see the words in context as well. And not just in an arbitrary context, but in the specific context from the course. Given that I had already gone from specific words to the word stem, I could go back to using those specific words to search for example sentences, add highlighting of the word in question, and include these in the file. Each card has a Russian word on the front, and when you turn it, you see the English translation (from Google Translate), the Russian stem (for reference), and up to three example sentences with the word highlighted.</p>\n\n<p><img src="/blog/images/2014-04-16-creating-anki-cards-for-russian-coursera-mooc-with-stemming-and-frequency-lists_-_whole-04.png" style="float:right; margin-left: 10px">\nI think the result is pretty awesome. It will be fun to practice on these cards, and then go back to watching the videos. When next week\'s videos are released, I should be able to quickly generate a list of words that are new (were not covered in week 1 or 2).</p>\n\n<p>There are of course many ways in which this could be improved. I\'m thinking of how to keep track of words I know, so that I can easily remove them when getting a word list from new material. Integrating this kind of functionality into the Coursera or other MOOC platforms would be neat, although it will probably never happen. (If they had more open APIs, other people could build it, however). I could for example easily extract the timing of the example sentences, and include that in the flash cards, so you could jump straight to a specific video at the point where the professor says a certain example sentence (however, I don\'t know how to generate URLs that open a Coursera video at a specific time).</p>\n\n<p>There\'s also a lot of other fun stuff one could do with subtitles - it would be great if we could get easy access to subtitles for all the courses, and use it to build a search engine (linked directly to videos etc.)</p>\n\n<p>But for now, I need to study some flashcards, and watch some Russian videos. ÐÐ¾ÐºÐ°!</p>\n\n<p><em>(All the code on <a href="https://github.com/houshuang/russian_stemmer">Github</a>)</em></p>',
          title:
            'Creating Anki cards for Russian Coursera MOOC with stemming and frequency lists',
          date: '2014-04-16T18:44:51.000Z',
          link:
            'http://reganmian.net/blog/blog/2014/04/16/creating-anki-cards-for-russian-coursera-mooc-with-stemming-and-frequency-lists/',
          categories: [],
          blogtitle: 'Random Stuff that Matters',
          author: 'Stian HÃ¥klev'
        }
      }
    }
  },
  cjid8bdy9000zc6se9kqce1yw: {
    id: 'cjid8bdy9000zc6se9kqce1yw',
    li: {
      id: 'cjid8bdy9000kc6ses5t0pxx2',
      liDocument: {
        liType: 'li-rss',
        createdAt: '2018-06-13T14:47:08.385Z',
        createdBy: 'op-rss',
        payload: {
          content:
            '<p>I got my introduction to functional programming through Clojure, but lately I\'ve been really fascinated by Haskell. I gave up on it a few times, because it can seem quite impenetrable, and very different from anything that I\'m used to. But there is something about the elegance, and powerful ideas that keeps me coming back. I\'ve read a bunch of tutorials and papers, followed blog posts, and experimented a bit with ghci and <a href="https://github.com/gibiansky/IHaskell">IHaskell</a>, but I never really got off the starting block with writing my own Haskell.</p>\n\n<h1>How to get practice?</h1>\n\n<p>The projects I\'m currently working on in Python are too complex and urgent for me to try to implement them in Haskell at this point, and because Haskell is so different, I found myself stymied doing even simple things, even though I\'d just finished reading a complex CS paper about monads. I needed some structured tasks that were not too hard, and that gave me good feedback on how to improve.</p>\n\n<p><a href="http://exercism.io/"><img src="/blog/images/2014-03-11-learning-idiomatic-haskell_-_whole-02.png" style="none"></a></p>\n\n<p>Listening to the <a href="http://www.functionalgeekery.com/">Functional Geekery podcast</a>, I heard about <a href="http://exercism.io/">exercism.io</a>, which provides exercises for many popular languages. There are several similar websites in existence, perhaps the most well known is <a href="https://projecteuler.net/">Project Euler</a>, however that focuses too much on CS/math-type problems, and is perhaps better for learning algorithms than a particular language. The way exercism.io works, is that you install it as a command line application. The first time you run it, it downloads the first exercise for a number of languages.</p>\n\n<p>The exercises themselves consist of test cases, and it\'s up to you to write a module that will fulfill the test case. Once you are done, you use the command line client to quickly submit the answer, which unlocks everyone else\'s answers, and downloads the next exercise. The fact that you download the test cases and README files, and work on your local computer, is also great. You can use the environment you\'re used to (Emacs, VIM, EclipseFP), with all the tooling, documentation, autorunning tests, etc., that you are used to.</p>\n\n<h1>Trying before being told</h1>\n\n<p>This kind of graded approach is brilliant, because it precludes "cheating". It\'s easy to skip ahead to the solution, say "that makes sense, I would have come up with that", and move on. But it\'s important to actually force yourself <em>to come up with that</em>. Reading other people\'s versions, and the feedback they received, after having struggled with the problem yourself, is also incredibly informative -- and again, I think you get far more out of it, than just browsing through solutions before you\'ve put in the hard work yourself.</p>\n\n<p><em>There\'s a parallel to some of the work I\'m doing with audience response systems in lectures. <a href="http://mazur.harvard.edu/research/detailspage.php?rowid=8">Eric Mazur at Harvard</a> puts physics problems on the projector, and asks students to first try to solve them themselves. Then they have to try to convince the person next to them about the right answer, which they submit by clicker. Finally, he explains and discusses. Students who have wrestled with the problem for a while already, and been exposed to other people\'s ways of thinking, are far more receptive and learn more from the final explanation, than if you had just gone directly to lecture.</em></p>\n\n<h1>Idiomatic code</h1>\n\n<p>The objective is not just to pass the tests (which is in most cases trivial, for example it would not be difficult to write very specific code that just fulfills the test cases, but does not generalize), but to write as beautiful, idiomatic code as possible. One might start with an inelegant approach that fulfills the test cases, and then begin refactoring into something one can be proud of, while making sure the test-cases continue to work.</p>\n\n<p>This is an interesting challenge, and it\'s also very useful, because one of the biggest challenges when learning a new language is internalizing the metaphors and idioms of that language. When writing a simple function, there might be many ways of writing it, that will all "work", and yield the same result. Yet once you begin writing larger and more complex programs, programs that need certain kinds of performance (memory, CPU, dealing with very large inputs etc), the difference between different approaches becomes very large.</p>\n\n<p>The first project I did was quite embarassing. The task asks you to construct a function that takes a string, and generates a string as a response. There are a bunch of test cases, and you have to figure out yourself how to group them (seems like strings ending with ? are questions, but not if they are all upper-case, etc). I decided to use pattern matching, which was a fair approach.</p>\n\n<p>However, I instinctively used regexp patterns to match the text, instead of looking for built-in functions. I got it to work mostly (and did learn how to use regexp in Haskell, which I hadn\'t used before), but I had problems with unicode, and spent too long trying different patterns. After I submitted, I saw an incredibly elegant solution, using only built-in predicates like <code>isAlphaNum</code>, which read and compose much better than my convoluted regexps. A very good lesson, and one I internalized much quicker than if I read a blog post about it, without having struggled with it myself first.</p>\n\n<p><img src="/blog/images/2014-03-11-learning-idiomatic-haskell_-_whole-01.png" style="none"></p>\n\n<h1>Future growth of exercism.io</h1>\n\n<p>The platform is quite young, and I\'m wondering how it will scale. For the first few exercises in Haskell, there were only about 15 responses (since you cannot proceed until you have submitted the first few responses, I\'m guessing the more advanced exercises will have even fewer answers). If there were hundred answers for a given problem, I might want to see as many different approaches as possible, not 20 examples that are basically the same (I\'m also interested in as much diverse feedback as possible).</p>\n\n<p>Perhaps some kind of voting feature is necessary. There is the possibility of providing feedback to individual submissions, and while I really appreciate people\'s feedback, I almost feel like it isn\'t necessary once I\'ve seen other people\'s contributions -- I already have a good idea of what I could do better. (There are only <em>so</em> many different ways of doing these fairly small and bounded problems).</p>\n\n<p>Anyway, I was very excited to discover this platform. After the first task, which I spent about two hours on, I did two more. With these, I was less embarassed when seeing other people\'s code, but in both cases, there were more elegant solutions than the one I suggested. One of the recurring themes also seems to be really understanding the core library, and all the functions that are already implemented, instead of trying to reinvent the wheel.</p>\n\n<p>Go learn some code!</p>',
          title: 'Learning idiomatic Haskell with Exercism.io',
          date: '2014-03-11T14:50:24.000Z',
          link:
            'http://reganmian.net/blog/blog/2014/03/11/learning-idiomatic-haskell/',
          categories: [],
          blogtitle: 'Random Stuff that Matters',
          author: 'Stian HÃ¥klev'
        }
      }
    }
  },
  cjid8bdy90010c6seble4bpls: {
    id: 'cjid8bdy90010c6seble4bpls',
    li: {
      id: 'cjid8bdy9000lc6se99kdwgxd',
      liDocument: {
        liType: 'li-rss',
        createdAt: '2018-06-13T14:47:08.385Z',
        createdBy: 'op-rss',
        payload: {
          content:
            '<p>I am currently working on analyzing MOOC clicklog data for a research project. The clicklogs themselves are huge text files (from 3GB to 20GB in size), where each line represents one "event" (a mouse click, pausing a video, submitting a quiz, etc). These events are represented as a (sometimes nested) JSON structure, which doesn\'t really have a clear schema.</p>\n\n<h1>Introduction</h1>\n\n<p>Our goal was to run frequent-sequence analyses over these clicklogs, but to do that, we needed to process them in two rounds. Initially, we walk throug the log-file, and convert each line (JSON blob) into a row in a <a href="http://pandas.pydata.org/">Pandas</a> table, which we store in a <a href="http://www.hdfgroup.org/HDF5/">HDF5</a> file using <a href="http://www.pytables.org/moin">pytables</a> (<a href="http://pandas.pydata.org/pandas-docs/dev/cookbook.html#cookbook-hdf">see also</a>). We convert the JSON key-values to columns, extract information from the URL (for example <code>/view/quiz?quiz_id=3</code> results in the column action receiving the value <code>/view/quiz</code>, and the column <code>quiz_id</code>, the value 3. We also do a bit of cleaning up of values, throw out some of the columns that are not useful, etc.</p>\n\n<h1>Speeding it up</h1>\n\n<p>We use Python for the processing, and even with a JSON parser written in C, this process is really slow. An obvious way of speeding it up would be to parallelize the code, taking advantage of the eight-cores on the server, rather than only maxing out a single core. I did not have much experience with parallel programming in general, or in Python, so this was a learning experience for me. I by no means consider myself an expert, and I might have missed some obvious things, but I still thought what we came up with might be useful to others.</p>\n\n<p>There are many approaches to parallelizing code, and choosing the right one requires thinking about your algorithms -- which functions need access to shared state (and can you design it so that you minimize shared state), are you CPU-bound, or IO-bound, what is the grain size that it makes sense to split at, etc.</p>\n\n<h1>Shared state</h1>\n\n<p>I was using HDF5 as my storage format, and pytables can append to an existing table. Theoretically, one would think that the lines in the log file are completely independent from each other, just get them converted and into the HDF5, and once you\'ve got them all entered (appended), you can index and sort by timestamp, username, etc. However, Pandas Data.Frames have one big limitation compared to R data.frames -- no categorical column type. In R, a data.frame might have a column that stores the answer to a likert-type question (<code>Not at all</code> to <code>Totally agree</code>, for example), and even though there are 10,000 rows, those five text answers are only stored once, with each cell only containing a pointer to the correct alternative. This makes the table take much less space.</p>\n\n<p>Pandas was developed for financial analysis, and still seems fairly focused on numbers. (It also builds upon NumPy, which also doesn\'t have a categorical format). In our case, this was a big problem, since the difference between storing "/view/lecture" (13 bytes) and 1 (1 byte) for a table with 14 million rows is huge! (Especially when you consider that Pandas does much of its work by loading the entire table into memory).</p>\n\n<p>I decided to hack my own little categorical format, by creating a memoize function, which would assign a unique number to each new value it was called with, but always return the same number for the same value. Quick example:</p>\n\n<pre><code class="python">memoize("hello") #       =&gt; 1\nmemoize("how are you") # =&gt; 2\nmemoize("hello") #       =&gt; 1\nmemoize("how are you") # =&gt; 2\nmemoize("i\'m fine") #    =&gt; 3\n</code></pre>\n\n<p>Initially the function simply kept state in a Pandas Series, and once I had finished parsing the entire clicklog, I wrote these series to the same HDF5 file (HDF5 files can contain multiple tables, files etc). This worked great, and it meant that instead of this:</p>\n\n<pre><code class="json">{"key": "user.video.lecture.action","value": "{\\"currentTime\\":257.066689,\n\\"playbackRate\\":1,\\"paused\\":false,\\"error\\":null,\\"networkState\\":1,\n\\"readyState\\":4,\\"eventTimestamp\\":13752243249891,\\"initTimestamp\\":\n137524554345511,\\"type\\":\\"ratechange\\",\\"prevTime\\":227.066689}","username":\n"9930a0523403499b7347707c92bccbcbff1a","timestamp": 13752234234277,"page_url":\n"https://class.mooc-platform.org/course-001/lecture/view?lecture_id=31","client":\n"spark","session":"30423409-1323423042304","language": "en-US,en;q=0.5","from":\n"https://class.mooc-platform.org/course-001/lecture/view?lecture_id=31",\n"user_ip": "123.255.122.167","user_agent": "Mozilla/5.0 (Windows NT 6.1; rv:22.0)\nGecko/20100101 Firefox/22.0", "12": ["{\\"height\\":678,\\"width\\":1207}"], "13": [0]}\n</code></pre>\n\n<p>...I ended up with what was externally represented as a list of 18 floats (I wanted to store them as integers instead, but Pandas needs floats to be able to represent NaNs, which was important, given that many of the values were sparse).</p>\n\n<h1>The problem with shared state</h1>\n\n<p>This worked well before I tried to do any kind of speedup (apart from the fact that the script took 12 hours to run on the largest file). However, I wanted to be able to iterate a little bit quicker, and explore ways of speeding up the code. Because I needed to keep the memoized data consistent, I couldn\'t just fork off a bunch of processes that each would process their own data -- they would have all ended up with their own individual memo tables, where <code>1</code> corresponds to <code>hello</code> in one process, and <code>what\'s up</code> in another. To address this, I moved the memoization code out of the function that processed individual lines, and applied it to each Pandas column in a final step before appending the processed chunks to the HDF5 file.</p>\n\n<p>The function to process each line should now be a pure function -- that is, a function that always returns the same output given the same input, not reliant on any external state, etc. -- and thus ripe for parallization. I tried using <code>pool.map</code> from the <a href="http://docs.python.org/3.3/library/multiprocessing.html">multiprocessing</a> library to let the computer automatically figure out how many external processes to run, etc. Here I ran into a problem of grain size... It takes a bit of time to fire up processes, transfer state, coordinate across threads etc, so if you assign each process too little work, the gain from having work done in parallel is more than lost. This was the result -- my naive attempt led to a slowdown. (Some libraries try to tackle this automatically and figure out the correct grain size, but pool.map didn\'t seem to be able to).</p>\n\n<p>My second attempt was to do the chunking myself -- I read in 100,000 lines, chunked it into chunks of 20,000 lines, and then mapped over this list of lists. This worked better, but I only saw a small speedup (maybe 50%). Partly because of the extra time spent chunking lines, and partly because a big part of the program was still spent converting the list of dicts to a Pandas DataFrame, memoizing the columns, and writing to file (I could see this very clearly by monitoring <code>top</code> -- first you see five processes maxing out their respective cores, and then suddenly they drop away, and there is only one process on one core working).</p>\n\n<h1>Map-reduce</h1>\n\n<p>A very common paradigm for cluster computing (with multiple computers) is called <a href="https://en.wikipedia.org/wiki/MapReduce">map-reduce</a>. Basically, you split the data into chunks, run parallel operations on each chunk, and then somehow "reduce" the result back together. An example could be getting number of page views per URL for an extremely large clicklog, have each serve count up clicks per URL for a piece of the log file, and then have a script that combines the numbers in the end.</p>\n\n<p>Something similar can be done on a single computer, and for the second phase of the analysis, this is what I ended up with. The second phase, after we got the nice HDF5 file, is to do some processing for each user. Instead of the fact that a user watched video 39, we want to know if he/she watched the video for the first time, or if it was an immediate review, to be able to compare patterns across students, and even across courses.</p>\n\n<p>This means that we cannot treat each <code>student event</code> individually -- we need to know which videos the students has already seen, etc. However, we can treat each <code>student</code> individually -- how one student\'s actions are tagged, are not at all reliant on other students\' actions.</p>\n\n<h1>Split, process, collect</h1>\n\n<p>One very nice thing about HDF5 is that once you\'ve indexed the table (which is quite fast), you can read in data based on simple queries. Thus, once we\'ve opened the HDF file with <code>store = pd.HDFStore("hdfstore.h5")</code>, we don\'t have to read the entire table into memory with <code>db = store[\'db\']</code>, which might take 6GB of memory (with a big table), but we can instead run</p>\n\n<pre><code class="python">user = store.select(\'db\', pd.Term(\'username = 3434\')\n</code></pre>\n\n<p>(in our case, we also select on timestamp, because we only want events that happened while the course was active). Add to this that pytables is thread-safe for reading (ie. multiple processes can read from the same file, without problems), but not for writing (it\'s not like a mysql database that can have multiple processes writing at the same time).</p>\n\n<p>So instead of having one script that tries to parallelize subtasks, what I ended up doing was to run eight Python scripts concurrently. I simply indicate as a command parameter which usernumber to start with, and how far to jump each time. Thus, the first Python process starts with user 1, then does user 9, user 17 etc, until there are no more users. The second process starts with user number 2, then 10, etc. This way all users get processes, without the need for any coordination. Initially I started the processes from a shell script, but then I put it into Python:</p>\n\n<pre><code class="python">print("Spawning %d processing scripts" % numprocesses)\nprocs = []\nfor proc in range(1, numprocesses+1):\n    range_start = proc\n    range_jump = numprocesses+1\n    procs.append(Popen([python_exec, "videos_reduce.py", hdffile, str(range_start), \\\n        str(range_jump), tmpdir, cutoff]))\n</code></pre>\n\n<p>I can then wait for all processes to finish, and then do any cleanup work that is necessary.</p>\n\n<pre><code class="python">def all_procs_finished(procs):\n    for proc in procs:\n        if proc.poll() == None:\n            return False\n        return True\n\nwhile not all_procs_finished(procs):\n    time.sleep(5)\n</code></pre>\n\n<h1>Reduce</h1>\n\n<p>Initially, I wanted to capture the output in a new HDF5 file, so I had the individual scripts dump their output pickled into temporary files (named using <a href="http://docs.python.org/3.3/library/uuid.html">uuid</a>) in a temporary directory, which would then be read by the main script, and appended to the HDF5 file (thus ensuring that only one thread wrote to the HDF5, to avoid data corruption). Later, we decided we wanted to output a text file for processing in R, which made it even easier. The individual thread simply spit out a bunch of text file chunks into a temporary directory, and then we combine them with <code>cat * &gt; outfile</code>.</p>\n\n<p>This approach worked swimmingly. Since we are just running the same script eight times in parallel, it\'s easy to debug by running the script only once (I would run it with <code>1 10000</code> to start with user 1 and jump 10000 ahead, to 10001, 20001 etc until the file had been exhausted). Because all the hard work was done in the individual threads, I got very close to 100% utilization on all eight cores for the duration of the run (which is a beauty to see in <code>htop</code>), and even when running a separate process that collected the output files and wrote them to HDF5, that process used so little CPU as to be negligible.</p>\n\n<p><img src="/blog/images/2014-03-10-parsing-massive-clicklogs-01.png" style="none"></p>\n\n<h1>But what about that shared state?</h1>\n\n<p>At this point, I needed to go back to the initial phase to redo a bunch of the parsing, because I had received new files. Inspired by my experience with the second phase, I thought about how I could improve the first-phase scripts. The tricky problem was how to coordinate state. I did a bunch of googling to look at "coordinated / parallel memoization libraries for Python", and somehow came upon people using <a href="http://redis.io/">Redis</a> for that purpose.</p>\n\n<p>I had heard very vaguely about Redis, but never known much about it. It turned out to be perfect for the job through. It\'s basically a very fast distributed key-value store (although it also has other value types, like lists and hashes), with an API that only takes a few hours to learn. I installed a Redis server, and rewrote my memoization function to use Redis instead of the Pandas Series, with only very minimal changes.</p>\n\n<p>Here is a shortened version of the memoization class:</p>\n\n<pre><code class="python">class Memo(object):\n    def __init__(self, name, prefix):\n        self.r = redis.StrictRedis(host=\'localhost\', decode_responses=True)\n        self.name = name\n        self.prefix = prefix\n        self.counter = \'%s:%s:cnt\' % (prefix, name)\n        self.store = \'%s:%s:store\' % (prefix, name)\n        if self.r.zcard(self.store) == 0:\n            self.add_object(\'None\')\n\n        def add_object(self, obj):\n            c = self.r.incr(self.counter)\n            self.r.zadd(self.store, c, obj)\n            return c\n\n        @lru_cache(maxsize=256)\n        def get(self, s):\n            if s is None or pd.isnull(s):\n                return -1\n\n                existing_score = self.r.zscore(self.store, s)\n                if existing_score:\n                    return existing_score\n                else:\n                    c = self.add_object(s)\n                    return c\n</code></pre>\n\n<p>You create a new Memo class with a name, and then just call it like described above. <code>lru_cache</code>, from <a href="http://docs.python.org/3.3/library/functools.html">functools</a>, is really neat - it remembers the last n (maxsize) arguments to the function, and their return values, and if you call the function with an argument in the list, it gives you the response, without having to run the function. This is safe, given that once a value is assigned in Redis, it will never change. Having this meant a big speedup (especially since it\'s very common for adjacent lines to have similar values, for example a chunk of lines all from the same user, etc), however when I tried to set <code>maxsize=None</code>, ie. to remember all the values, it became much slower. Apparently Redis is much faster at looking up a hash-value among 10,000 than Python, despite the overhead of calling out.</p>\n\n<p>So the final version of the script does several thing. It begins by running the shell command <code>split -n</code>, which splits the log file into chunks based on number of lines (currently 40,000 lines seems to be a good size). It does not wait until cut is finished, but begins immediately launching the 8 processes, which begin reading the next available file in the temp dir, and processing it.</p>\n\n<p>The eight workers pick up a temp file "chew on it", and spit out the result as a pickled array of dicts in another temp dir. The master script, having launched all the processes, begins to cycle, waiting for files to appear, reads them in, and appends them to the HDF5 file. The worker processes continue until they\'ve gotten the signal that cut has finished (through Redis), and there are no more files. The master script continues until all worker processes have finished, and there are no more "chewed" files. It then repacks the HDF5 file (to add indices), and adds the memoized data from Redis.</p>\n\n<p>Using this approach, I am now able to process a 20GB clicklog file in 12 minutes! (Resulting in a 3.5 GB HDF5 file with something like 14 million events).</p>',
          title: 'Parsing massive clicklogs, an approach to parallel Python',
          date: '2014-03-10T14:09:17.000Z',
          link:
            'http://reganmian.net/blog/blog/2014/03/10/parsing-massive-clicklogs/',
          categories: [],
          blogtitle: 'Random Stuff that Matters',
          author: 'Stian HÃ¥klev'
        }
      }
    }
  },
  cjid8bdya0011c6se7ffs1qs9: {
    id: 'cjid8bdya0011c6se7ffs1qs9',
    li: {
      id: 'cjid8bdy9000mc6setm7f8gb2',
      liDocument: {
        liType: 'li-rss',
        createdAt: '2018-06-13T14:47:08.385Z',
        createdBy: 'op-rss',
        payload: {
          content:
            "<p>I haven't been blogging for months, and there is a lot of things I'd like to write about. But rather than waiting until I have the time to do that, I thought I'd just quickly capture a neat function that many might not know about.</p>\n\n<p>Running shell commands on multiple files is something we do every day, usually with different wildcard patterns (like <code>rm *</code>, which deletes all the files in the current directory). What's sometimes not quite clear to me is when the wildcard-expansion happens by the shell (and ie. <code>rm</code> is given hundreds of arguments), and when it is passed to the command to do it's own expansion.</p>\n\n<p>But sometimes you want to do something on files in multiple directories. Some shells like zsh will let you do <code>**/*.md</code> to list all Markdown files, arbitrarily nested, whereas in Bash, this only goes down one directory. An alternative to this is to first generate a list of the files, and then execute the command once for each file. <code>xargs</code> can be used to do this (although I always somehow found its syntax a bit difficult). A great alternative is <code>gnu parallel</code>, which does the same, but in parallel. Since most current computers have four, eight, or even more cores, this can speed things up significantly. (And even more so if the function needs to wait for a network connection, for example pinging hosts, downloading files with curl, etc).</p>\n\n<p>My concrete problem was that I received some zip files with clickstream logs. These zip files had thousands of log files, in nested sub-directories, all individually encrypted with my gpg public key. I needed to traverse all the directories, decrypt the files, and remove the originally encrypted files, before I begin parsing them.</p>\n\n<p><code>parallel</code> has detailed <a href=\"http://www.gnu.org/software/parallel/parallel_tutorial.html\">documentation</a>, and I'm just scratching the surface in terms of functionality. Because I needed to do two actions (decrypt, and then delete the original file), I chose to create a small bash script that would be run once for each file. This has the advantage that I can easily test this on a single file, and make sure all the arguments are correct etc. Because I also read in my gpg password from the command line, I obviate the need to remember a complex command, but don't have to store my gpg password in a file (on a shared server).</p>\n\n<p>Here's my quick script:</p>\n\n<pre><code class=\"bash\">#!/bin/bash\necho $1|gpg --batch --passphrase-fd 0 --decrypt-files $2 &amp;&gt; /dev/null\nrm $2\n</code></pre>\n\n<p>I can try to run it on a single file first, and once I am happy that it works properly, I can fire away:</p>\n\n<pre><code class=\"bash\">find . | grep gpg$ | parallel --progress -j 8 ./decrypt.sh MY-PASS-PHRASE\n</code></pre>\n\n<p>This uses find . to get a list of all files in the current directory and all subdirectories, and grep to select those ending in gpg. For file A, it would then execute <code>./decrypt.sh MY-PASS-PHRASE A</code>. <code>-j 8</code> specifies that it should run 8 jobs in parallel (the server has 8 cores), and <code>--progress</code> shows a simple progress meter. Here's how it looks when I run it:</p>\n\n<pre><code class=\"bash\">Computers / CPU cores / Max jobs to run\n1:local / 8 / 8\n\nComputer:jobs running/jobs completed/%of started jobs/Average seconds to complete\nlocal:1/886/100%/0.1s\n</code></pre>\n\n<p>In a few seconds, it decrypted 886 files for me. Very quick to write, quick to repeat in the future, quick to execute -- what's not to love? (From the status above, you see <code>computers</code>, which indicates that <code>parallel</code> can also be used across multiple computers. Read the instruction for more advanced usage.</p>",
          title: 'GNU Parallel, quick and easy',
          date: '2014-03-09T18:36:29.000Z',
          link:
            'http://reganmian.net/blog/blog/2014/03/09/gnu-parallel-quick-and-easy/',
          categories: [],
          blogtitle: 'Random Stuff that Matters',
          author: 'Stian HÃ¥klev'
        }
      }
    }
  }
};

export const meta = {
  name: 'Common Knowledge board',
  mode: 'collab',
  shortDesc: '2D board for placing items',
  description:
    'All imported items are placed on a 2D space. Optionally, teacher can designate four named quadrants. Students can drag boxes to organize or group ideas. Incoming items have title and content.',
  exampleData: [
    {
      title: 'Board',
      config: { quadrants: false },
      data
    },
    {
      title: 'Quadrants and boxes',
      config: {
        quadrants: true,
        quadrant1: 'Capitalism',
        quadrant2: 'Socialism',
        quadrant3: 'Modernism',
        quadrant4: 'Post-modernism'
      },
      data
    },
    {
      title: 'Background image',
      config: {
        quadrants: false,
        image: true,
        imageurl: '/file?name=ac/ac-ck-board/researchCycle.png'
      },
      data
    }
  ]
};

export const config = {
  type: 'object',
  properties: {
    allowCreate: { title: 'Enable adding new Learning Items', type: 'boolean' },
    image: {
      title: 'Display background image',
      type: 'boolean'
    },
    imageurl: {
      title: 'URL of background image',
      type: 'string'
    },
    quadrants: {
      title: 'Draw four quadrants, named as below',
      type: 'boolean'
    },
    quadrant1: {
      title: 'Quadrant 1 title',
      type: 'string'
    },
    quadrant2: {
      title: 'Quadrant 2 title',
      type: 'string'
    },
    quadrant3: {
      title: 'Quadrant 3 title',
      type: 'string'
    },
    quadrant4: {
      title: 'Quadrant 4 title',
      type: 'string'
    }
  }
};

export const configUI = {
  quadrant1: { conditional: 'quadrants' },
  quadrant2: { conditional: 'quadrants' },
  quadrant3: { conditional: 'quadrants' },
  quadrant4: { conditional: 'quadrants' },
  imageurl: { conditional: 'image' }
};

export const validateConfig = [
  (data: Object): null | { field?: string, err: string } =>
    data.image && data.quadrants
      ? {
          err:
            'You cannot have both a background image and quadrants at the same time'
        }
      : null
];
